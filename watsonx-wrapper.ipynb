{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper WatsonX v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, List, Optional, Sequence, Type, Union, Callable, Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import LLMResult, Generation, GenerationChunk\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"Extended IBM watsonx.ai large language models.\"\"\"\n",
    "\n",
    "    bound_tools: Optional[List[BaseTool]] = Field(default=None, exclude=True)\n",
    "\n",
    "    def __init__(self, *args, tools: Optional[List[BaseTool]] = None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.bound_tools = tools or []\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[Any] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Call the IBM watsonx.ai inference endpoint which then generate the response.\n",
    "        Args:\n",
    "            prompts: List of strings (prompts) to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "            run_manager: Optional callback manager.\n",
    "            stream: Optional boolean flag to indicate streaming generation.\n",
    "        Returns:\n",
    "            The full LLMResult output.\n",
    "        \"\"\"\n",
    "        params = self._get_chat_params(stop=stop)\n",
    "        should_stream = stream if stream is not None else self.streaming\n",
    "        if should_stream:\n",
    "            if len(prompts) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"WatsonxLLM currently only supports single prompt, got {prompts}\"\n",
    "                )\n",
    "            generation = GenerationChunk(text=\"\")\n",
    "            stream_iter = self._stream(\n",
    "                prompts[0], stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            for chunk in stream_iter:\n",
    "                if generation is None:\n",
    "                    generation = chunk\n",
    "                else:\n",
    "                    generation += chunk\n",
    "            assert generation is not None\n",
    "            if isinstance(generation.generation_info, dict):\n",
    "                llm_output = generation.generation_info.pop(\"llm_output\")\n",
    "                return LLMResult(generations=[[generation]], llm_output=llm_output)\n",
    "            return LLMResult(generations=[[generation]])\n",
    "        else:\n",
    "            # Apply tools before generation\n",
    "            if self.bound_tools:\n",
    "                tool_output = self._evaluate_tools(self.bound_tools, prompts[0])\n",
    "                print(\"Tool output: %s\", tool_output)\n",
    "                logger.info(\"Tool output: %s\", tool_output)\n",
    "                # You can include the tool output in the prompt or handle it as needed\n",
    "                # For simplicity, we concatenate it to the prompt here\n",
    "                #prompts[0] += \"\\n\\n\" + tool_output\n",
    "\n",
    "\n",
    "                # Create a system prompt using the tool output\n",
    "                system_prompt = (\n",
    "                    f\"You are an assistant with access to web search results. \"\n",
    "                    f\"Provide a detailed answer to the user's query.\\n\\n\"\n",
    "                    f\"User Query: {prompts[0]}\\n\\n\"\n",
    "                    f\"Using the information below:\\n\"\n",
    "                    f\"Web Search Results: {tool_output}\\n\\n\"\n",
    "                )\n",
    "\n",
    "                # Set the new prompt with the system prompt first\n",
    "                prompts[0] = system_prompt\n",
    "                    \n",
    "  \n",
    "\n",
    "            print(\"prompts\",prompts)\n",
    "\n",
    "            response = self.watsonx_model.generate(\n",
    "                prompt=prompts, params=params, **kwargs\n",
    "            )\n",
    "            return self._create_llm_result(response)\n",
    "\n",
    "    def _evaluate_tools(self, tool_instances: List[BaseTool], input_text: str) -> str:\n",
    "        \"\"\"Evaluate the provided tools and return their combined output.\n",
    "        Args:\n",
    "            tool_instances: List of tools to evaluate.\n",
    "            input_text: Input text to provide to the tools.\n",
    "        Returns:\n",
    "            Combined output of the tool evaluations.\n",
    "        \"\"\"\n",
    "        combined_output = []\n",
    "        for tool in tool_instances:\n",
    "            result = tool.invoke(input_text)\n",
    "            #print(\"WebSearch Results:\",result)\n",
    "            #content = \"\\n\\n\".join([f\"Content: {item['content']}\" for item in result])\n",
    "            content= \"WebSearch Results: \" + \" \".join(result['content'] for result in result)\n",
    "            combined_output.append(content)\n",
    "        return \"\\n\\n\".join(combined_output)\n",
    "\n",
    "    @classmethod\n",
    "    def bind_tools(\n",
    "        cls,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[Union[Dict[str, str], Literal[\"any\", \"auto\"], str]] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> 'WatsonxLLM':\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "        Assumes model is compatible with OpenAI tool-calling API.\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type\": \"any\"},\n",
    "                        or {\"type\": \"auto\"};\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "        formatted_tools = [convert_to_openai_tool(tool)[\"function\"] for tool in tools]\n",
    "        instance = cls(**kwargs)\n",
    "        instance.bound_tools = tools\n",
    "        if tool_choice is not None:\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        return instance\n",
    "\n",
    "    def _create_llm_result(self, response: List[dict]) -> LLMResult:\n",
    "        \"\"\"Create the LLMResult from the choices and prompts.\"\"\"\n",
    "        generations = []\n",
    "        for res in response:\n",
    "            results = res.get(\"results\")\n",
    "            if results:\n",
    "                finish_reason = results[0].get(\"stop_reason\")\n",
    "                gen = Generation(\n",
    "                    text=results[0].get(\"generated_text\"),\n",
    "                    generation_info={\"finish_reason\": finish_reason},\n",
    "                )\n",
    "                generations.append([gen])\n",
    "        final_token_usage = self._extract_token_usage(response)\n",
    "        llm_output = {\n",
    "            \"token_usage\": final_token_usage,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"deployment_id\": self.deployment_id,\n",
    "        }\n",
    "        return LLMResult(generations=generations, llm_output=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "load_dotenv()  # Load environment variables from .env file\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Create an instance of WatsonxLLM\n",
    "# WatsonxLLM initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_instance  = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompts ['How is the weather in Genova']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Genoa has a moderate climate with hot summers and mild winters. \\n\\n\\nThe average temperature in Genoa in January is 7.2°C, while in July it is 25.7°C. The rainiest months are October and November, while the sunniest are July and August.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watsonx_instance.invoke(\"How is the weather in Genova\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage:\n",
    "tool = TavilySearchResults(max_results=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools = watsonx_instance.bind_tools(tools=[tool],model_id=model_id,url=url,apikey=api_key,project_id=project_id,params=parameters)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool output: %s WebSearch Results: Ruslan Magana Vsevolodovna. National Institute for Nuclear Physics. Verified email at ge.infn.it - Homepage. Nuclear Physics Machine Learning Data Science Cloud Computing Big Data. ... R Magana, H Zheng, A Bonasera. International Journal of Modern Physics E 21 (01), 1250006, 2012. 4: 2012: I'm Ruslan Magana Vsevolodovna. I'm a Data Scientist, a Cloud Architect and a Physicist. About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track ... Nov 2020. Ruslan Magaña Vsevolodovna. The structure of odd-A Rh115,117 and Pd115,117 isotopes is studied by means of the neutron-proton interacting boson-fermion model (IBFM-2). JP=12+ quantum ... I am Data Scientist and Data Engineer. I have a Ph.D. in Physics and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "prompts [\"You are an assistant with access to web search results. Provide a detailed answer to the user's query.\\n\\nUser Query: Who is Ruslan Magana?\\n\\nUsing the information below:\\nWeb Search Results: WebSearch Results: Ruslan Magana Vsevolodovna. National Institute for Nuclear Physics. Verified email at ge.infn.it - Homepage. Nuclear Physics Machine Learning Data Science Cloud Computing Big Data. ... R Magana, H Zheng, A Bonasera. International Journal of Modern Physics E 21 (01), 1250006, 2012. 4: 2012: I'm Ruslan Magana Vsevolodovna. I'm a Data Scientist, a Cloud Architect and a Physicist. About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track ... Nov 2020. Ruslan Magaña Vsevolodovna. The structure of odd-A Rh115,117 and Pd115,117 isotopes is studied by means of the neutron-proton interacting boson-fermion model (IBFM-2). JP=12+ quantum ... I am Data Scientist and Data Engineer. I have a Ph.D. in Physics and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\\n\\n\"]\n",
      "generations=[[Generation(text='Ruslan Magana Vsevolodovna is a Nuclear Physics Researcher working at the National Institute for Nuclear Physics in Italy. Ruslan Magana is also a Data Scientist, Cloud Architect and Physicist based in the US with an extensive background in Artificial Intelligence, Generative AI and prompt engineering.', generation_info={'finish_reason': 'eos_token'})]] llm_output={'token_usage': {'generated_token_count': 67, 'input_token_count': 297}, 'model_id': 'ibm/granite-13b-instruct-v2', 'deployment_id': ''} run=None\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_tools._generate(prompts=[\"Who is Ruslan Magana?\"])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool output: %s WebSearch Results: I am Data Scientist and Data Engineer. I have a Ph.D. in Physics and I am AWS certified in Machine Learning and Data Analytics - ruslanmv Ruslan Magana Vsevolodovna. National Institute for Nuclear Physics. Verified email at ge.infn.it - Homepage. Nuclear Physics Machine Learning Data Science Cloud Computing Big Data. ... R Magana, H Zheng, A Bonasera. International Journal of Modern Physics E 21 (01), 1250006, 2012. 4: 2012: Nov 2020. Ruslan Magaña Vsevolodovna. The structure of odd-A Rh115,117 and Pd115,117 isotopes is studied by means of the neutron-proton interacting boson-fermion model (IBFM-2). JP=12+ quantum ... I'm Ruslan Magana Vsevolodovna. I'm a Data Scientist, a Cloud Architect and a Physicist. About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track ...\n",
      "prompts [\"You are an assistant with access to web search results. Provide a detailed answer to the user's query.\\n\\nUser Query: Who is Ruslan Magana?\\n\\nUsing the information below:\\nWeb Search Results: WebSearch Results: I am Data Scientist and Data Engineer. I have a Ph.D. in Physics and I am AWS certified in Machine Learning and Data Analytics - ruslanmv Ruslan Magana Vsevolodovna. National Institute for Nuclear Physics. Verified email at ge.infn.it - Homepage. Nuclear Physics Machine Learning Data Science Cloud Computing Big Data. ... R Magana, H Zheng, A Bonasera. International Journal of Modern Physics E 21 (01), 1250006, 2012. 4: 2012: Nov 2020. Ruslan Magaña Vsevolodovna. The structure of odd-A Rh115,117 and Pd115,117 isotopes is studied by means of the neutron-proton interacting boson-fermion model (IBFM-2). JP=12+ quantum ... I'm Ruslan Magana Vsevolodovna. I'm a Data Scientist, a Cloud Architect and a Physicist. About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track ...\\n\\n\"]\n",
      "Ruslan Magana Vsevolodovna is a Data Scientist and Data Engineer with a Ph.D. in Physics and AWS certified in Machine Learning and Data Analytics. Ruslan Magana Vsevolodovna is the Head of the Nuclear Physics group at the National Institute for Nuclear Physics.\n"
     ]
    }
   ],
   "source": [
    "response = llm_with_tools.invoke(\"Who is Ruslan Magana?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
