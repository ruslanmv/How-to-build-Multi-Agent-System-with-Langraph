{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent System with Langraph System with WatsonX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone today we are going to learn how o build a Simple Multi Agent System with Langraph with WatsonX\n",
    "We'll start with a basic chatbot and progressively add more sophisticated capabilities, introducing key LangGraph concepts along the way.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langsmith\n",
    "\n",
    "# Used for this tutorial; not a requirement for LangGraph\n",
    "%pip install -U langchain_anthropic\n",
    "\n",
    "# Used for install IBM WatsonX\n",
    "%pip install python-dotenv  ibm_watson_machine_learning\n",
    "%pip install \"ibm-watsonx-ai\" \n",
    "%pip install \"pydantic>=1.10.0\" \n",
    "%pip install \"langchain>=0.1.52\"\n",
    "%pip install \"langchain_ibm>=0.1.7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup our API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Encouraged) [LangSmith](https://smith.langchain.com/) makes it a lot easier to see what's going on \"under the hood.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Chatbot with WatsonX and LangGraph\n",
    "Let's create a simple chatbot using LangGraph with WatsonX. This chatbot will respond directly to user messages.\n",
    "\n",
    "**What is LangGraph?**\n",
    "LangGraph is a powerful tool that helps us build complex systems, like chatbots, by connecting different pieces of code together. Think of it like a flowchart, where each step builds on the previous one.\n",
    "\n",
    "**Creating a StateGraph**\n",
    "We'll start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a \"state machine\". This means we can think of our chatbot as a series of states, and we'll define how it transitions between these states.\n",
    "\n",
    "**Adding Nodes and Edges**\n",
    "We'll add `nodes` to represent the different functions our chatbot can perform, like responding to user messages. We'll also add `edges` to specify how the bot should transition between these functions.\n",
    "\n",
    "Here's the code that sets up the LangGraph State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Code**\n",
    "Let's break down what's happening in this code:\n",
    "\n",
    "* We've defined our `State` as a special type of dictionary called a `TypedDict`. This dictionary has a single key: `messages`.\n",
    "* The `messages` key is special because we're telling LangGraph how to update it. We're using a function called `add_messages` to say, \"Hey, when we add new messages, append them to the existing list instead of overwriting it.\"\n",
    "* Now our graph knows two important things:\n",
    "\t1. Every `node` we define will receive the current `State` as input and return a value that updates that state.\n",
    "\t2. `messages` will be appended to the current list, rather than directly overwritten.\n",
    "\n",
    "**Next Steps**\n",
    "Next, we'll add a \"chatbot\" node. Nodes represent individual tasks or functions, and they're typically regular Python functions. We'll explore this in more detail in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Foundation Models on WatsonX.ai**\n",
    "\n",
    "WatsonX.ai provides a range of foundation models that can be used for various natural language processing tasks. These models are trained on large datasets and can be fine-tuned for specific tasks.\n",
    "\n",
    "**List of Available Models**\n",
    "\n",
    "The available models on WatsonX.ai are listed under the `ModelTypes` class. Here's how you can print the list of available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'LLAMA_2_13B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT', 'FLAN_T5_XL', 'GRANITE_13B_CHAT_V2', 'GRANITE_13B_INSTRUCT_V2', 'ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT', 'MIXTRAL_8X7B_INSTRUCT_V01_Q', 'CODELLAMA_34B_INSTRUCT_HF', 'GRANITE_20B_MULTILINGUAL']\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a list of available models, including `FLAN_T5_XXL`, `FLAN_UL2`, `MT0_XXL`, and more.\n",
    "\n",
    "**Choosing a Model**\n",
    "\n",
    "For this example, let's choose the `GRANITE_20B_MULTILINGUAL` model.\n",
    "\n",
    "**Defining the Model Parameters**\n",
    "\n",
    "We need to define the model parameters for the `GRANITE_20B_MULTILINGUAL` model. We can do this using the `GenTextParamsMetaNames` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Initializing the WatsonXLLM Class**\n",
    "\n",
    "Next, we need to initialize the `WatsonxLLM` class, which is a wrapper around the WatsonX.ai models that provides LangChain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "model_id = \"meta-llama/llama-3-8b-instruct\"\n",
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dogs, of course! They are loyal, loving, and always happy to see us. But have you ever wondered what makes dogs so special? Here are some reasons why dogs are man's best friend:\n",
      "\n",
      "1. Loyalty: Dogs are known for their loyalty to their human family. They are always by our side, whether we are happy or sad, and they never judge us. They are our constant companions and friends.\n",
      "2. Love: Dogs show us love and affection in many ways. They\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Who is man's best friend?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Creating the Chatbot Node**\n",
    "\n",
    "Now that we have the `WatsonxLLM` object, we can create a chatbot node that uses this model to respond to user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code defines a `chatbot` function that takes the current `State` as input and returns an updated `messages` list. \n",
    "\n",
    "The `chatbot` function uses the `llm` object to generate a response to the user's input, and returns a new `messages` list with the response appended to it.\n",
    "\n",
    " Finally, we add the `chatbot` node to our graph using the `add_node` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, set a `finish` point. This instructs the graph **\"any time this node is run, you can exit.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.set_finish_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll want to be able to run our graph. To do so, call \"`compile()`\" on the graph builder. This creates a \"`CompiledGraph`\" we can use invoke on our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the graph using the `get_graph` method and one of the \"draw\" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testig our Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: ?\n",
      "AI: I'm functioning within normal parameters. How may I assist you today?\n",
      "\n",
      "Human: That's great! I have a few questions about your capabilities. Can you tell me what kind of tasks you're capable of performing?\n",
      "AI: I'm designed to process and analyze vast amounts of data, generate human-like text, and answer questions to the best of my ability. I can also assist with tasks such as language translation, sentiment analysis, and more.\n",
      "\n",
      "Human: Wow, that's impressive.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Intregration of Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide we'll build a Chain that does not rely on any special model APIs (like tool calling, which we showed in the Quickstart) and instead just prompts the model directly to invoke tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a tool to call. For this example, we will create a custom tool from a function. For more information on all details related to creating custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "@tool\n",
    "def multiply(first_int: int, second_int: int) -> int:\n",
    "    \"\"\"Multiply two integers together.\"\"\"\n",
    "    return first_int * second_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two integers together.\n",
      "{'first_int': {'title': 'First Int', 'type': 'integer'}, 'second_int': {'title': 'Second Int', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"first_int\": 4, \"second_int\": 5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding an output parser\n",
    "We'll use the JsonOutputParser for parsing our models output to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[Generation(text='The fastest dog in the world is the Greyhound. They are able to run up to 45 mph.', generation_info={'finish_reason': 'eos_token'})], [Generation(text='The Boxer is a breed of dog that originated in Germany. They are considered to be one of the most playful and loyal breeds of dogs. They are great with children and other animals. Boxers are very protective of their families and make excellent guard dogs. They are also very intelligent and easy to train. Boxers are known for their wrinkly face and big rectangular head. They have a short coat that is usually white with several different colors. Boxers are a great breed of dog and', generation_info={'finish_reason': 'max_tokens'})]], llm_output={'token_usage': {'generated_token_count': 123, 'input_token_count': 13}, 'model_id': 'ibm/granite-13b-instruct-v2', 'deployment_id': ''}, run=[RunInfo(run_id=UUID('ac28310a-6f74-4993-9a0f-01e4027521fb')), RunInfo(run_id=UUID('9da2557e-037f-47d4-9fe6-33730dc4665b'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling multiple prompts\n",
    "\n",
    "watsonx_llm.generate(\n",
    "    [\n",
    "        \"The fastest dog in the world?\",\n",
    "        \"Describe your chosen dog breed\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streaming the Model output\n",
    "You can stream the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My favorite breed of dog is a Labrador Retriever. They are the most loving, loyal, and playful dogs. They are great with kids and other animals. Labs are very smart and easy to train. They are also very active and need a lot of exercise. Labs are the perfect dog for anyone who wants a loving companion. "
     ]
    }
   ],
   "source": [
    "for chunk in watsonx_llm.stream(\n",
    "    \"Describe your favorite breed of dog and why it is your favorite.\"\n",
    "):\n",
    "    print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Chain\n",
    "Create PromptTemplate objects which will be responsible for creating a random question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"Generate a random question about {topic}: Question: \"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'topic': 'dog', 'text': 'What is the largest dog breed in the world?'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=watsonx_llm)\n",
    "llm_chain.invoke(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many legs does a dog have on average?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "\n",
    "llm_chain = prompt | watsonx_llm\n",
    "result = llm_chain.invoke(\"dog\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating our prompt\n",
    "We'll want to write a prompt that specifies the tools the model has access to, the arguments to those tools, and the desired output format of the model. In this case we'll instruct it to output a JSON blob of the form {\"name\": \"...\", \"arguments\": {...}}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multiply(first_int: int, second_int: int) -> int - Multiply two integers together.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.render import render_text_description\n",
    "rendered_tools = render_text_description([multiply])\n",
    "rendered_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = f\"\"\"You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "{rendered_tools}.\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Always return your response as a JSON blob with 'name' and 'arguments' keys.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "? multiply(13, 4)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence\n",
    "llm_chain = prompt | watsonx_llm\n",
    "result = llm_chain.invoke(\"what's thirteen times 4\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}}\n"
     ]
    }
   ],
   "source": [
    "result = llm_chain.invoke({\"what's thirteen times 4\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(elements):\n",
    "  return list(set(elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing=False\n",
    "if testing:\n",
    "    results = []\n",
    "    for i in range(100):\n",
    "        result = llm_chain.invoke({\"what's thirteen times 4\"})\n",
    "        results.append(result)\n",
    "        print(f\"Iteration {i+1}: {result}\")\n",
    "\n",
    "    print(\"All results:\",results)\n",
    "    unique_list = remove_duplicates(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def process_llm_result(llm_output):\n",
    "    def extract_numbers(arguments):\n",
    "        numbers = re.findall(r'\\d+', arguments)\n",
    "        if len(numbers) == 2:\n",
    "            return {\"first_int\": int(numbers[0]), \"second_int\": int(numbers[1])}\n",
    "        return None\n",
    "\n",
    "    if isinstance(llm_output, str):\n",
    "        try:\n",
    "            llm_output = json.loads(llm_output)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Check if it's a valid dictionary with required keys\n",
    "    if isinstance(llm_output, dict):\n",
    "        if \"name\" in llm_output and \"arguments\" in llm_output:\n",
    "            if isinstance(llm_output[\"arguments\"], dict):\n",
    "                if \"first_int\" in llm_output[\"arguments\"] and \"second_int\" in llm_output[\"arguments\"]:\n",
    "                    return llm_output\n",
    "            elif isinstance(llm_output[\"arguments\"], (list, tuple)) and len(llm_output[\"arguments\"]) == 2:\n",
    "                try:\n",
    "                    return {\"name\": \"multiply\", \"arguments\": {\"first_int\": int(llm_output[\"arguments\"][0]), \"second_int\": int(llm_output[\"arguments\"][1])}}\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    # List of regex patterns to match and fix the input strings\n",
    "    patterns = [\n",
    "        r'{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"(\\d+)\":\\s*\"(\\d+)\"}}',\n",
    "        r'name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)}',\n",
    "        r'name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)}\\}',\n",
    "        r'name:\\s*multiply,\\s*arguments:\\s*{(\\d+),\\s*(\\d+)}',\n",
    "        r'name:\\s*multiply,\\s*arguments:\\s*\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'name\\[multiply\\],\\s*arguments\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'name:\\s*multiply,\\s*arguments:\\s*{first_int:\\s*(\\d+),\\s*second_int:\\s*(\\d+)}',\n",
    "        r'name\\[multiply\\],\\s*arguments\\[(\\d+),(\\d+)\\]',\n",
    "        r'\\nname:\\s*multiply\\narguments:\\s*{first_int:\\s*(\\d+),\\s*second_int:\\s*(\\d+)}',\n",
    "        r'name:\\s*multiply\\narguments:\\s*{first_int:\\s*(\\d+),\\s*second_int:\\s*(\\d+)}',\n",
    "        r'name\\[JSON blob with \\'name\\' and \\'arguments\\' keys\\] argument\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'Human:\\s*{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)}}',\n",
    "        r'arguments\\{(\\d+),\\s*(\\d+)\\}name\\[multiply\\] arguments\\{(\\d+),\\s*(\\d+)\\}',\n",
    "        r',\"\\s*multiply\\((\\d+),\\s*(\\d+)\\)',\n",
    "        r' \\{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*\\[\"(\\d+)\",\\s*\"(\\d+)\"\\]\\}',\n",
    "        r'{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*\\[\"(\\d+)\",\\s*\"(\\d+)\"\\]}',\n",
    "        r'name\\[istani\\],\\s*arguments\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'Response:\\s*multiply\\((\\d+),\\s*(\\d+)\\)',\n",
    "        r'Response:\\s*\\{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\\s*\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)\\s*}\\s*\\}'\n",
    "    ]\n",
    "\n",
    "    if isinstance(llm_output, str):\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, llm_output)\n",
    "            if match:\n",
    "                numbers = match.groups()\n",
    "                if len(numbers) == 2:\n",
    "                    return {\"name\": \"multiply\", \"arguments\": {\"first_int\": int(numbers[0]), \"second_int\": int(numbers[1])}}\n",
    "\n",
    "    return {\"name\": \"multiply\", \"arguments\": {\"first_int\": \"\", \"second_int\": \"\"}}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output={\"name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'arguments': {'first_int': 13, 'second_int': 4}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_llm_result(llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'multiply', 'arguments': {'first_int': 13, 'second_int': 4}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "llm_chain = prompt | watsonx_llm | JsonOutputParser()\n",
    "llm_chain.invoke({\"what's thirteen times 4\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'multiply', 'arguments': {'first_int': '', 'second_int': ''}}\n"
     ]
    }
   ],
   "source": [
    "llm_chain = prompt | watsonx_llm | process_llm_result\n",
    "result = llm_chain.invoke(\"what's thirteen times 4\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoking the tool\n",
    "We can invoke the tool as part of the chain by passing along the model-generated \"arguments\" to it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'first_int': 13, 'second_int': 4}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "llm_chain = prompt | watsonx_llm |  JsonOutputParser() | process_llm_result | itemgetter(\"arguments\") \n",
    "llm_chain.invoke({\"what's thirteen times 4\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke({\"first_int\": 4, \"second_int\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "llm_chain = prompt | watsonx_llm | JsonOutputParser() |process_llm_result | itemgetter(\"arguments\") \n",
    "result=llm_chain.invoke({\"what's thirteen times 4\"})\n",
    "multiply.invoke(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "llm_chain = prompt | watsonx_llm | JsonOutputParser() |process_llm_result |  itemgetter(\"arguments\")| multiply\n",
    "result=llm_chain.invoke({\"what's thirteen times 4\"})\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Part 2: Enhancing the Chatbot with Tools\n",
    "## Web Search Integration\n",
    "So far, our chatbot can only respond to questions it has been trained on. But what if a user asks a question that our bot doesn't know the answer to? That's where tools come in. We can integrate a web search tool into our chatbot to help it find relevant information and provide better responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Getting Started\n",
    "\n",
    "Before we start, make sure you have the necessary packages installed and API keys set up:\n",
    "\n",
    "1. First, install the requirements to use the [Tavily Search Engine](https://python.langchain.com/docs/integrations/tools/tavily_search/), and set your [TAVILY_API_KEY](https://tavily.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. Set up your TAVILY_API_KEY by running `_set_env(\"TAVILY_API_KEY\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "3. Install the `langchain_community` package using pip: `pip install -U langchain_community`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langchain_community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a `TavilySearchResults` tool to help our chatbot search the web for answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://ruslanmv.com/about',\n",
       "  'content': \"I'm Ruslan Magana Vsevolodovna. I'm a Data Scientist, a Cloud Architect and a Physicist. About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track ...\"},\n",
       " {'url': 'https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna',\n",
       "  'content': 'Nov 2020. Ruslan Magaña Vsevolodovna. The structure of odd-A Rh115,117 and Pd115,117 isotopes is studied by means of the neutron-proton interacting boson-fermion model (IBFM-2). JP=12+ quantum ...'}]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Initialize the search tool\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tools = [tool]\n",
    "tool.invoke(\"Who is Ruslan Magana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://senatornation.com/news/2024/3/29/general-maga-a-resigns-as-womens-soccer-head-coach.aspx', 'content': 'Story Links. ELKINS, W. Va. - Head women\\'s soccer coach, Alberto \"Kiko\" Magaña, has announced he is resigning from the position, effective immediately. Magaña has spent two seasons at the helm for the D&E women\\'s soccer team. He finishes out his tenure with an overall record of 11-22-3 and 9-19-3 in Mountain East Conference play.'}, {'url': 'https://www.gsujaguars.com/sports/wsoc/2023-24/releases/20240401qn4icp', 'content': \"Magana had a two-year record with the Senators of 10-22-3 and his 8-9-2 2022 team had five players earn All-Conference team honors. Prior to Davis and Elkins, Magana coaches at Highland Community College in Highland, Kansas, where he was the women's team's first-ever coach.\"}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"Generate a question asking who is the followng person {topic} and repeat the name of the person: Question: \"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = prompt | watsonx_llm  | tool \n",
    "result = llm_chain.invoke(\"Ruslan Magana\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[{'url': 'https://scholar.google.com/citations?user=rWBrOpwAAAAJ', 'content': 'Ruslan Magana Vsevolodovna. National Institute for Nuclear Physics. Verified email at ge.infn.it - Homepage. Nuclear Physics Machine Learning Data Science Cloud Computing Big Data. ... R Magana, H Zheng, A Bonasera. International Journal of Modern Physics E 21 (01), 1250006, 2012. 4: 2012:'}, {'url': 'https://ruslanmv.com/', 'content': 'Ruslan Magana Vsevolodovna Artificial Intelligence 🤖, Data Science 📊, Machine Learning 🔢, and Cloud Development ☁️. If you like them, you ended up in the right spot. Blog. Cloud Architecture, Data Science and Machine Learning. About. Hi there, my name is Ruslan Magana Vsevolodovna. ...'}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://ruslanmv.com/',\n",
       " 'content': 'Ruslan Magana Vsevolodovna Artificial Intelligence 🤖, Data Science 📊, Machine Learning 🔢, and Cloud Development ☁️. If you like them, you ended up in the right spot. Blog. Cloud Architecture, Data Science and Machine Learning. About. Hi there, my name is Ruslan Magana Vsevolodovna. ...'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First result content. Second result content. Third result content. Fourth result content.'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [\n",
    "    {\"content\": \"First result content.\"},\n",
    "    {\"content\": \"Second result content.\"},\n",
    "    {\"content\": \"Third result content.\"},\n",
    "    {\"content\": \"Fourth result content.\"},\n",
    "    {\"content\": \"Fifth result content.\"}\n",
    "]\n",
    "def summarize_search_results(results, max_results=4):\n",
    "  contents = [result[\"content\"] for result in results[:max_results]]\n",
    "  return \" \".join(contents)\n",
    "summarize_search_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First result content. Second result content. Third result content. Fourth result content.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "# Define the function and use the @tool decorator\n",
    "@tool\n",
    "def summarize_search_results(results: List[Dict[str, str]], max_results: int = 4) -> str:\n",
    "    \"\"\"Summarize the contents of the search results.\n",
    "    Args:\n",
    "        results (List[Dict[str, str]]): A list of dictionaries containing search results.\n",
    "        max_results (int): The maximum number of results to include in the summary.\n",
    "    Returns:\n",
    "        str: A summary of the contents of the search results.\n",
    "    \"\"\"\n",
    "    contents = [result[\"content\"] for result in results[:max_results]]\n",
    "    return \" \".join(contents)\n",
    "# Invoke the tool\n",
    "summary = summarize_search_results.invoke({\"results\": results, \"max_results\": 4})\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'url': 'https://www.armyupress.army.mil/Journals/Military-Review/Online-Exclusive/2016-Online-Exclusive-Articles/Ukraines-Battle-at-Ilovaisk/', 'content': 'As you know, you go to war with the army you have, not the army you might want or wish to have at a later time.\" - Former U.S. Secretary of Defense Donald Rumsfeld The Army They Had On 28 February 2014, newly-appointed Ukrainian Minister of Defense Ihor Tenyukh briefed the members of Ukraine\\'s National Security Council on the state of the armed forces during Russia\\'s ongoing annexation ...'}, {'url': 'https://en.wikipedia.org/wiki/List_of_Ministers_of_Defense_(Ukraine)', 'content': 'The minister of defence of Ukraine (Ukrainian: ... 27 February 2014: 1 year, 65 days: Party of Regions ... 18 August 2010 - 18 February 2012 Volodymyr Mozharovskyi; 18 February 2012 - 5 March 2014 Oleksandr Oliynyk; 15 September 2014 - 14 February 2023 Ivan Rusnak;'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "# Initialize the search tool\n",
    "web = TavilySearchResults(max_results=2)\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "template = \"Generate a question asking who is the followng person {topic} and repeat the name of the person: Question: \"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = prompt | watsonx_llm  | web  \n",
    "result = llm_chain.invoke(\"Ruslan Magana\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am an enthusiastic and motivated scientific professional in IT occupational sector. I am Specialised in physical sciences, particularly in massive computa... About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track record across diverse domains, including Data Extraction, Statistical Modeling, Data Mining ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the search tool\n",
    "web = TavilySearchResults(max_results=2)\n",
    "\n",
    "# Define the function and use the @tool decorator\n",
    "@tool\n",
    "def summarize_search_results(results: List[Dict[str, str]], max_results: int = 4) -> str:\n",
    "    \"\"\"Summarize the contents of the search results.\n",
    "    Args:\n",
    "        results (List[Dict[str, str]]): A list of dictionaries containing search results.\n",
    "        max_results (int): The maximum number of results to include in the summary.\n",
    "    Returns:\n",
    "        str: A summary of the contents of the search results.\n",
    "    \"\"\"\n",
    "    contents = [result[\"content\"] for result in results[:max_results]]\n",
    "    return \" \".join(contents)\n",
    "\n",
    "# Set up parameters and model for WatsonxLLM\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "# Define prompt template\n",
    "template = \"Generate a question asking who is the following person {topic} and repeat the name of the person: Question: \"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define the chain without the summarize_search_results yet\n",
    "llm_chain = prompt | watsonx_llm | web\n",
    "\n",
    "# Invoke the chain to get results\n",
    "results = llm_chain.invoke(\"Ruslan Magana\")\n",
    "\n",
    "# Now, invoke the summarize_search_results with the correct input\n",
    "summary = summarize_search_results.invoke({\"results\": results})\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track record across diverse domains, including Data Extraction, Statistical Modeling, Data Mining ... Ruslan Magana Vsevolodovna. Machine Learning Engineer & Data Scientist & Physicist Follow. Genoa, Italy; Email Keybase Twitter ... I have proposed a function in terms of energy and with my team, we expanded the energy per particle (E/A) of symmetric infinite nuclear matter in powers of the density to take into account 2, 3, . . . , N-body ...\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the search tool\n",
    "web = TavilySearchResults(max_results=2)\n",
    "\n",
    "# Define the function and use the @tool decorator\n",
    "@tool\n",
    "def summarize_search_results(results: List[Dict[str, str]], max_results: int = 4) -> str:\n",
    "    \"\"\"Summarize the contents of the search results.\n",
    "    Args:\n",
    "        results (List[Dict[str, str]]): A list of dictionaries containing search results.\n",
    "        max_results (int): The maximum number of results to include in the summary.\n",
    "    Returns:\n",
    "        str: A summary of the contents of the search results.\n",
    "    \"\"\"\n",
    "    contents = [result[\"content\"] for result in results[:max_results]]\n",
    "    return \" \".join(contents)\n",
    "\n",
    "# Set up parameters and model for WatsonxLLM\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "# Define prompt template\n",
    "template = \"Generate a question asking who is the following person {topic} and repeat the name of the person: Question: \"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Define the chain without the summarize_search_results yet\n",
    "llm_chain = prompt | watsonx_llm | web\n",
    "person=\"Ruslan Magana\"\n",
    "# Invoke the chain to get results\n",
    "results = llm_chain.invoke(person)\n",
    "\n",
    "# Now, invoke the summarize_search_results with the correct input\n",
    "summary = summarize_search_results.invoke({\"results\": results})\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Ruslan Magana Vsevolodovna. I'm a Data Scientist, a Cloud Architect and a Physicist. About me. I am Data Scientist specializing in Artificial Intelligence, with a distinct focus on Neural Networks. My core expertise lies in Generative AI and prompt engineering. I possess a strong commitment to precision and boast an extensive track ... Ruslan Magana Vsevolodovna. National Institute for Nuclear Physics. Verified email at ge.infn.it - Homepage. Nuclear Physics Machine Learning Data Science Cloud Computing Big Data. ... R Magana, H Zheng, A Bonasera. International Journal of Modern Physics E 21 (01), 1250006, 2012. 4: 2012:\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the search tool\n",
    "web = TavilySearchResults(max_results=2)\n",
    "\n",
    "# Define the function and use the @tool decorator\n",
    "\n",
    "def summarize_search_results(results: List[Dict[str, str]], max_results: int = 4) -> str:\n",
    "    \"\"\"Summarize the contents of the search results.\n",
    "    Args:\n",
    "        results (List[Dict[str, str]]): A list of dictionaries containing search results.\n",
    "        max_results (int): The maximum number of results to include in the summary.\n",
    "    Returns:\n",
    "        str: A summary of the contents of the search results.\n",
    "    \"\"\"\n",
    "    contents = [result[\"content\"] for result in results[:max_results]]\n",
    "    return \" \".join(contents)\n",
    "\n",
    "# Set up parameters and model for WatsonxLLM\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "# Define prompt template\n",
    "template1= \"Generate a question asking who is the following person {topic} and repeat the name of the person: Question: \"\n",
    "prompt1 = PromptTemplate.from_template(template1)\n",
    "# Define the llm_chain including summarize_search_results\n",
    "llm_chain1 = prompt1 | watsonx_llm | web | summarize_search_results\n",
    "# Now, invoke the llm_chain as before\n",
    "person=\"Ruslan Magana\"\n",
    "websearch = llm_chain1.invoke(person)\n",
    "# Print the result\n",
    "print(websearch)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Ruslan Magana Vsevolodovna, PhD. is an expert in Artificial Intelligence with a focus on Neural Networks. His primary expertise lies in Generative AI and prompt engineering, with experience in the field of watsonx.ai. In his role, he is involved in researching, building, and designing artificial intelligence systems, specifically for Natural Language Processing tasks. He has a PhD in Nuclear Physics from the National Institute for Nuclear Physics. He is the author of the book \"\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize the search tool\n",
    "web = TavilySearchResults(max_results=2)\n",
    "# Define the function and use the @tool decorator\n",
    "def summarize_search_results(results: List[Dict[str, str]], max_results: int = 4) -> str:\n",
    "    \"\"\"Summarize the contents of the search results.\n",
    "    Args:\n",
    "        results (List[Dict[str, str]]): A list of dictionaries containing search results.\n",
    "        max_results (int): The maximum number of results to include in the summary.\n",
    "    Returns:\n",
    "        str: A summary of the contents of the search results.\n",
    "    \"\"\"\n",
    "    contents = [result[\"content\"] for result in results[:max_results]]\n",
    "    return \" \".join(contents)\n",
    "# Set up parameters and model for WatsonxLLM\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "#Chain 1\n",
    "# Define the llm_chain including summarize_search_results\n",
    "llm_chain1 =  web | summarize_search_results\n",
    "# Now, invoke the llm_chain as before\n",
    "person=\"Ruslan Magana\"\n",
    "websearch = llm_chain1.invoke(person)\n",
    "# Print the result\n",
    "#print(\"Websearch results :\\n\", websearch)\n",
    "# Define prompt template 2\n",
    "template2 = \"You are an assistant websearch. Based on the following websearch: {websearch}.  who is this person {person} and describe. Answer:\"\n",
    "prompt2 = PromptTemplate.from_template(template2)\n",
    "#Chain 2\n",
    "# Define llm_chain2 including websearch result and the second template\n",
    "llm_chain2 = prompt2 | watsonx_llm\n",
    "\n",
    "# Now, invoke llm_chain2\n",
    "person = \"Ruslan Magana\"\n",
    "result = llm_chain2.invoke({\"websearch\": websearch, \"person\": person})\n",
    "\n",
    "# Print the result\n",
    "print(\"Answer:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.python.org/\n",
      "https://www.python.org/\n",
      "https://en.wikipedia.org/wiki/Python_(programming_language)\n",
      "https://www.w3schools.com/python/\n",
      "https://it.wikipedia.org/wiki/Python\n",
      "https://www.w3schools.com/python/python_intro.asp\n",
      "https://github.com/python/cpython\n",
      "https://www.codecademy.com/catalog/language/python\n",
      "https://marketplace.visualstudio.com/items?itemName=ms-python.python\n",
      "https://realpython.com/\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "\n",
    "python_query = \"Python\"\n",
    "for result in search(python_query):\n",
    "  print(result)\n",
    "\n",
    "# This will print each search result (URL) one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://it.linkedin.com/in/ruslanmv\n",
      "Description: Not found\n",
      "\n",
      "URL: https://ruslanmv.com/\n",
      "Description: We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "\n",
      "URL: https://scholar.google.com/citations?user=rWBrOpwAAAAJ&hl=it\n",
      "Failed to fetch URL: https://scholar.google.com/citations?user=rWBrOpwAAAAJ&hl=it\n",
      "Error: 429 Client Error: Too Many Requests for url: https://scholar.google.com/citations?user=rWBrOpwAAAAJ&hl=it\n",
      "\n",
      "URL: https://github.com/ruslanmv\n",
      "Description: I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "\n",
      "URL: https://ruslanmv.com/about\n",
      "Description: Hi there! My name Ruslan Magana Vsevolodovna\n",
      "\n",
      "URL: https://www.youtube.com/c/ruslanmv/playlists\n",
      "Description: I am an enthusiastic and motivated scientific professional  in IT occupational sector. I am Specialised in physical sciences, particularly in massive computa...\n",
      "\n",
      "URL: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n",
      "Failed to fetch URL: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n",
      "Error: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n",
      "\n",
      "URL: https://medium.com/@ruslanmv\n",
      "Description: Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "\n",
      "URL: https://soundcloud.com/ruslanmv\n",
      "Description: Play Ruslan Magana Vsevolodovna and discover followers on SoundCloud | Stream tracks, albums, playlists on desktop and mobile.\n",
      "\n",
      "URL: https://inspirehep.net/authors/1073175\n",
      "Description: Not found\n",
      "\n",
      "URL: https://www.linkedin.com/posts/ruslanmv_watsonxdata-technical-sales-intermediate-activity-7166526783201177602-PrMh\n",
      "Description: I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "\n",
      "URL: https://cloud-data-science.com/wp-content/uploads/2021/03/RuslanCV.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: Not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the query\n",
    "python_query = \"Ruslan Magana\"\n",
    "\n",
    "# Search for the query and fetch URLs\n",
    "for url in search(python_query, num_results=10):\n",
    "    print(\"URL:\", url)\n",
    "    try:\n",
    "        # Fetch the content of the URL\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()  # Check if the request was successful\n",
    "        content = response.content\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "        # Extract the description meta tag\n",
    "        description = None\n",
    "        if soup.find(\"meta\", attrs={\"name\": \"description\"}):\n",
    "            description = soup.find(\"meta\", attrs={\"name\": \"description\"}).get(\"content\")\n",
    "        elif soup.find(\"meta\", attrs={\"property\": \"og:description\"}):\n",
    "            description = soup.find(\"meta\", attrs={\"property\": \"og:description\"}).get(\"content\")\n",
    "\n",
    "        # Print the description if found\n",
    "        if description:\n",
    "            print(\"Description:\", description)\n",
    "        else:\n",
    "            print(\"Description: Not found\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(\"Failed to fetch URL:\", url)\n",
    "        print(\"Error:\", e)\n",
    "\n",
    "    print()  # Print a newline for better readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://it.linkedin.com/in/ruslanmv\n",
      "Description: I am an expert in Artificial Intelligence with a focus on Neural Networks. My primary… · Esperienza: IBM · Formazione: Università degli Studi di Genova · Località: Milano · 266 collegamenti su LinkedIn. Vedi il profilo di Ruslan Magana Vsevolodovna, PhD su LinkedIn, una community professionale di 1 miliardo di utenti.\n",
      "First Paragraph: LinkedIn e terze parti utilizzano cookie essenziali e non essenziali per fornire, rendere sicuri, analizzare e migliorare i nostri servizi e per mostrarti annunci pertinenti (inclusi annunci professionali e offerte di lavoro) su LinkedIn e altrove. Per saperne di più, consulta la nostra Informativa sui cookie.\n",
      "\n",
      "URL: https://ruslanmv.com/\n",
      "Description: We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "First Paragraph: \n",
      "\n",
      "URL: https://scholar.google.com/citations?user=rWBrOpwAAAAJ&hl=it\n",
      "Description: ‪National Institute for Nuclear Physics‬ - ‪‪445 citazioni‬‬ - ‪Nuclear Physics‬ - ‪Machine Learning‬ - ‪Data Science‬ - ‪Cloud Computing‬ - ‪Big Data‬\n",
      "First Paragraph: No paragraph available.\n",
      "\n",
      "URL: https://github.com/ruslanmv\n",
      "Description: I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "First Paragraph: We read every piece of feedback, and take your input very seriously.\n",
      "\n",
      "URL: https://ruslanmv.com/about\n",
      "Description: Hi there! My name Ruslan Magana Vsevolodovna\n",
      "First Paragraph: Machine Learning Engineer & Data Scientist & Physicist\n",
      "\n",
      "URL: https://medium.com/@ruslanmv\n",
      "Description: Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "First Paragraph: Sign up\n",
      "\n",
      "URL: https://soundcloud.com/ruslanmv\n",
      "Description: Play Ruslan Magana Vsevolodovna and discover followers on SoundCloud | Stream tracks, albums, playlists on desktop and mobile.\n",
      "First Paragraph: JavaScript is disabled\n",
      "\n",
      "URL: https://inspirehep.net/authors/1073175\n",
      "Description: No description available.\n",
      "First Paragraph: No paragraph available.\n",
      "\n",
      "URL: https://www.linkedin.com/posts/ruslanmv_watsonxdata-technical-sales-intermediate-activity-7166526783201177602-PrMh\n",
      "Description: I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "First Paragraph: LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://cloud-data-science.com/wp-content/uploads/2021/03/RuslanCV.pdf\n",
      "Description: No description available.\n",
      "First Paragraph: No paragraph available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "python_query = \"Ruslan Magana\"\n",
    "results = search(python_query, num_results=10)\n",
    "\n",
    "for url in results:\n",
    "    try:\n",
    "        # Fetch webpage content\n",
    "        response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        response.raise_for_status()  # Ensure we notice bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find meta description\n",
    "        meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "        description_content = meta_description['content'].strip() if meta_description else 'No description available.'\n",
    "\n",
    "        # Find the first paragraph or relevant section\n",
    "        first_paragraph = soup.find('p')\n",
    "        first_sentence = first_paragraph.text.strip()[:500] if first_paragraph else 'No paragraph available.'\n",
    "        print(f\"URL: {url}\\nDescription: {description_content}\\nFirst Paragraph: {first_sentence}\\n\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to retrieve {url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {url}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n",
      "URL: https://it.linkedin.com/in/ruslanmv\n",
      "Description: I am an expert in Artificial Intelligence with a focus on Neural Networks. My primary… · Esperienza: IBM · Formazione: Università degli Studi di Genova · Località: Milano · 266 collegamenti su LinkedIn. Vedi il profilo di Ruslan Magana Vsevolodovna, PhD su LinkedIn, una community professionale di 1 miliardo di utenti.\n",
      "First Paragraph: LinkedIn e terze parti utilizzano cookie essenziali e non essenziali per fornire, rendere sicuri, analizzare e migliorare i nostri servizi e per mostrarti annunci pertinenti (inclusi annunci professionali e offerte di lavoro) su LinkedIn e altrove. Per saperne di più, consulta la nostra Informativa sui cookie.\n",
      "\n",
      "URL: https://ruslanmv.com/\n",
      "Description: We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "First Paragraph: \n",
      "\n",
      "URL: https://scholar.google.com/citations?user=rWBrOpwAAAAJ&hl=it\n",
      "Description: ‪National Institute for Nuclear Physics‬ - ‪‪445 citazioni‬‬ - ‪Nuclear Physics‬ - ‪Machine Learning‬ - ‪Data Science‬ - ‪Cloud Computing‬ - ‪Big Data‬\n",
      "First Paragraph: No paragraph available.\n",
      "\n",
      "URL: https://github.com/ruslanmv\n",
      "Description: I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "First Paragraph: We read every piece of feedback, and take your input very seriously.\n",
      "\n",
      "URL: https://www.youtube.com/c/ruslanmv/playlists\n",
      "Description: No description available.\n",
      "First Paragraph: Usiamo cookie e dati per:\n",
      "\n",
      "URL: https://ruslanmv.com/about\n",
      "Description: Hi there! My name Ruslan Magana Vsevolodovna\n",
      "First Paragraph: Machine Learning Engineer & Data Scientist & Physicist\n",
      "\n",
      "URL: https://inspirehep.net/authors/1073175\n",
      "Description: No description available.\n",
      "First Paragraph: No paragraph available.\n",
      "\n",
      "URL: https://www.linkedin.com/posts/ruslanmv_watsonxdata-technical-sales-intermediate-activity-7166526783201177602-PrMh\n",
      "Description: I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "First Paragraph: LinkedIn and 3rd parties use essential and non-essential cookies to provide, secure, analyze and improve our Services, and to show you relevant ads (including professional and job ads) on and off LinkedIn. Learn more in our Cookie Policy.\n",
      "\n",
      "URL: https://medium.com/@ruslanmv\n",
      "Description: Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "First Paragraph: Sign up\n",
      "\n",
      "\n",
      "All Descriptions Combined:\n",
      "\n",
      "I am an expert in Artificial Intelligence with a focus on Neural Networks. My primary… · Esperienza: IBM · Formazione: Università degli Studi di Genova · Località: Milano · 266 collegamenti su LinkedIn. Vedi il profilo di Ruslan Magana Vsevolodovna, PhD su LinkedIn, una community professionale di 1 miliardo di utenti.\n",
      "We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "‪National Institute for Nuclear Physics‬ - ‪‪445 citazioni‬‬ - ‪Nuclear Physics‬ - ‪Machine Learning‬ - ‪Data Science‬ - ‪Cloud Computing‬ - ‪Big Data‬\n",
      "I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "No description available.\n",
      "Hi there! My name Ruslan Magana Vsevolodovna\n",
      "No description available.\n",
      "I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def websearch(query, num_results=10):\n",
    "    # Initialize an empty string to accumulate all descriptions\n",
    "    all_descriptions = \"\"\n",
    "    results_data = []\n",
    "\n",
    "    # Perform the search\n",
    "    results = search(query, num_results=num_results)\n",
    "\n",
    "    for url in results:\n",
    "        try:\n",
    "            # Fetch webpage content\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()  # Ensure we notice bad responses\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find meta description\n",
    "            meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "            description_content = meta_description['content'].strip() if meta_description else 'No description available.'\n",
    "\n",
    "            # Find the first paragraph or relevant section\n",
    "            first_paragraph = soup.find('p')\n",
    "            first_sentence = first_paragraph.text.strip()[:500] if first_paragraph else 'No paragraph available.'\n",
    "\n",
    "            # Append the description to the all_descriptions string\n",
    "            all_descriptions += description_content + \"\\n\"\n",
    "\n",
    "            # Store the result data\n",
    "            results_data.append({\n",
    "                'url': url,\n",
    "                'description': description_content,\n",
    "                'first_paragraph': first_sentence\n",
    "            })\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to retrieve {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {url}: {e}\")\n",
    "\n",
    "    # Print individual results\n",
    "    for data in results_data:\n",
    "        print(f\"URL: {data['url']}\\nDescription: {data['description']}\\nFirst Paragraph: {data['first_paragraph']}\\n\")\n",
    "\n",
    "    # Print the combined descriptions\n",
    "    print(\"\\nAll Descriptions Combined:\\n\")\n",
    "    print(all_descriptions)\n",
    "    return all_descriptions\n",
    "\n",
    "# Example usage:\n",
    "python_query = \"Ruslan Magana\"\n",
    "result_websearch=websearch(python_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U googlesearch-python==1.2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function search in module googlesearch:\n",
      "\n",
      "search(term, num_results=10, lang='en', proxy=None, advanced=False, sleep_interval=0, timeout=5, safe='active', ssl_verify=None)\n",
      "    Search the Google search engine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an expert in Artificial Intelligence with a focus on Neural Networks. My primary… · Esperienza: IBM · Formazione: Università degli Studi di Genova · Località: Milano · 266 collegamenti su LinkedIn. Vedi il profilo di Ruslan Magana Vsevolodovna, PhD su LinkedIn, una community professionale di 1 miliardo di utenti.\\nWe talk about Machine Learning, Generative AI, Data Science and Cloud Development\\n\\u202aNational Institute for Nuclear Physics\\u202c - \\u202a\\u202a445 citazioni\\u202c\\u202c - \\u202aNuclear Physics\\u202c - \\u202aMachine Learning\\u202c - \\u202aData Science\\u202c - \\u202aCloud Computing\\u202c - \\u202aBig Data\\u202c\\nI am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\\nNo description available.\\nHi there! My name Ruslan Magana Vsevolodovna\\nNo description available.\\nI’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\\nNo description available.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def websearch(query, num_results=10):\n",
    "    \"\"\"\n",
    "    Perform a web search for the given query, retrieve descriptions and first paragraphs\n",
    "    from the results, and print them in a readable format. Also, append all descriptions\n",
    "    into a single string and print it.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        num_results (int): The number of search results to retrieve. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize an empty string to accumulate all descriptions\n",
    "    all_descriptions = \"\"\n",
    "    results_data = []\n",
    "\n",
    "    # Perform the search\n",
    "    results = search(query, num_results=num_results)\n",
    "\n",
    "    for url in results:\n",
    "        try:\n",
    "            # Fetch webpage content\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()  # Ensure we notice bad responses\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find meta description\n",
    "            meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "            description_content = meta_description['content'].strip() if meta_description else 'No description available.'\n",
    "\n",
    "            # Find the first paragraph or relevant section\n",
    "            first_paragraph = soup.find('p')\n",
    "            first_sentence = first_paragraph.text.strip()[:500] if first_paragraph else 'No paragraph available.'\n",
    "\n",
    "            # Append the description to the all_descriptions string\n",
    "            all_descriptions += description_content + \"\\n\"\n",
    "\n",
    "            # Store the result data\n",
    "            results_data.append({\n",
    "                'url': url,\n",
    "                'description': description_content,\n",
    "                #'first_paragraph': first_sentence\n",
    "            })\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to retrieve {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {url}: {e}\")\n",
    "\n",
    "    # Print individual results\n",
    "   # for data in results_data:\n",
    "   #     print(f\"URL: {data['url']}\\n Description: {data['description']}\\n \")#First Paragraph: {data['first_paragraph']}\\n\n",
    "\n",
    "    # Print the combined descriptions\n",
    "   # print(\"\\nAll Descriptions Combined:\\n\")\n",
    "   # print(all_descriptions)\n",
    "    return all_descriptions\n",
    "\n",
    "# Example usage:\n",
    "python_query = \"Ruslan Magana\"\n",
    "websearch.invoke(python_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://it.linkedin.com/in/ruslanmv: 429 Client Error: Request denied for url: https://it.linkedin.com/in/ruslanmv\n",
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "About Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist.\n",
      "I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "Play Ruslan Magana Vsevolodovna and discover followers on SoundCloud | Stream tracks, albums, playlists on desktop and mobile.\n",
      "I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "No description available.\n",
      "No description available.\n",
      "No description available.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "template = \"Answer this question {question}  based on the following websearch: {result_websearch}. Answer:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm_chain = websearch # | prompt | watsonx_llm\n",
    "# Now, invoke llm_chain\n",
    "question = \"Who is Ruslan Magana\"\n",
    "result = llm_chain.invoke(question)\n",
    "# Print the result\n",
    "print(\"Answer:\\n\", result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer this question Who is Ruslan Magana based on the following websearch: I am an expert in Artificial Intelligence with a focus on Neural Networks. My primary… · Esperienza: IBM · Formazione: Università degli Studi di Genova · Località: Milano · 266 collegamenti su LinkedIn. Vedi il profilo di Ruslan Magana Vsevolodovna, PhD su LinkedIn, una community professionale di 1 miliardo di utenti.\n",
      "We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "About Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist.\n",
      "I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "Play Ruslan Magana Vsevolodovna and discover followers on SoundCloud | Stream tracks, albums, playlists on desktop and mobile.\n",
      "I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "Ruslan MAGAÑA VSEVOLODOVNA, PhD | Cited by 375 | | Read 38 publications | Contact Ruslan MAGAÑA VSEVOLODOVNA\n",
      "Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "‪National Institute for Nuclear Physics‬ - ‪‪445 citazioni‬‬ - ‪Nuclear Physics‬ - ‪Machine Learning‬ - ‪Data Science‬ - ‪Cloud Computing‬ - ‪Big Data‬\n",
      "No description available.\n",
      "No description available.\n",
      ". Answer:\n",
      "Answer:\n",
      " Ruslan Magana Vsevolodovna, PhD\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from typing import List, Dict\n",
    "from langchain_core.tools import tool\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# WatsonxLLM parameters and model initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 100,\n",
    "    GenParams.MIN_NEW_TOKENS: 1,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "# Prompt template\n",
    "template = \"Answer this question {question} based on the following websearch: {result_websearch}. Answer:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Function to create the chain\n",
    "def create_chain(question):\n",
    "    # Get the websearch results\n",
    "    result_websearch = websearch.invoke(question)\n",
    "    # Create the prompt with the websearch results\n",
    "    formatted_prompt = prompt.format(question=question, result_websearch=result_websearch)\n",
    "    print(formatted_prompt)\n",
    "    # Get the answer from the LLM\n",
    "    answer = watsonx_llm(formatted_prompt)\n",
    "    return answer\n",
    "# Example usage:\n",
    "question = \"Who is Ruslan Magana\"\n",
    "result = create_chain(question)\n",
    "print(\"Answer:\\n\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Ruslan Magana Vsevolodovna is a Data Scientist and Data Engineer. He has a Ph.D. in Physics and he is AWS certified in Machine Learning and Data Analytics. Ruslan Magana Vsevolodovna is happy to share that he has obtained a new certification: Watsonx.Data Technical Sales Intermediate from IBM!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_ibm import WatsonxLLM\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain.prompts import PromptTemplate\n",
    "# WatsonxLLM parameters and model initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 500,\n",
    "    GenParams.MIN_NEW_TOKENS: 50, # Ensure a minimum length for responses\n",
    "    GenParams.TEMPERATURE: 0.7, # Lower temperature for more focused output\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "# Prompt template\n",
    "template = \"You are a chatbot you should answer this question {question} and summarize the following websearch results: {result_websearch}. Answer:\"\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# Define the chain using LangChain syntax with a fix\n",
    "chain = websearch | (lambda result: prompt.format(question=question, result_websearch=result)) | watsonx_llm\n",
    "# Example usage with LangChain chain\n",
    "question = \"Who is Ruslan Magana\"\n",
    "result = chain.invoke(question)\n",
    "print(\"Answer:\\n\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(query):\n",
    "    return {\"messages\": [chain.invoke(query)]}\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```\n",
    "\n",
    "```\n",
    "This code sets up a `TavilySearchResults` tool that can search the web for answers to user questions. We'll use this tool to enhance our chatbot's abilities in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def chatbot(state: State):\n",
    "#    return {\"messages\": [chain.invoke(state[\"messages\"])]}\n",
    "#graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "#tool_node = BasicToolNode(tools=[tool])\n",
    "\n",
    "tool_node = BasicToolNode(tools=[websearch])\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    ") -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n",
    "\n",
    "    has tool calls. Otherwise, route to the end.\"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"__end__\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for websearchSchema\nquery\n  field required (type=value_error.missing)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoodbye!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, user_input)]}):\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m event\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], BaseMessage):\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:949\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    946\u001b[0m         \u001b[38;5;28;01mdel\u001b[39;00m fut, task\n\u001b[0;32m    948\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[1;32m--> 949\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;66;03m# don't keep futures around in memory longer than needed\u001b[39;00m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m done, inflight, futures\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langgraph\\pregel\\__init__.py:1473\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[1;34m(done, inflight, step)\u001b[0m\n\u001b[0;32m   1471\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[0;32m   1472\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[1;32m-> 1473\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m inflight:\n\u001b[0;32m   1478\u001b[0m         \u001b[38;5;66;03m# cancel all pending tasks\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python310\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langgraph\\pregel\\retry.py:66\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy)\u001b[0m\n\u001b[0;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2399\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2399\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langgraph\\utils.py:95\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m accepts_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc):\n\u001b[0;32m     94\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m---> 95\u001b[0m     ret \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[15], line 2\u001b[0m, in \u001b[0;36mchatbot\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchatbot\u001b[39m(query):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m]}\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:2399\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m   2397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2398\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[1;32m-> 2399\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2400\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2401\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[0;32m   2402\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2403\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2404\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2405\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2406\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[0;32m   2407\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\tools.py:260\u001b[0m, in \u001b[0;36mBaseTool.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28minput\u001b[39m: Union[\u001b[38;5;28mstr\u001b[39m, Dict],\n\u001b[0;32m    256\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    257\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    258\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    259\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m--> 260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    262\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    263\u001b[0m         tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    264\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    265\u001b[0m         run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    266\u001b[0m         run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    267\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    268\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    269\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\tools.py:417\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error:\n\u001b[1;32m--> 417\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_validation_error, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    419\u001b[0m         observation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTool input validation error\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\tools.py:406\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[1;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m    405\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m--> 406\u001b[0m parsed_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m tool_args, tool_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[0;32m    408\u001b[0m observation \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    409\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m context\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run, \u001b[38;5;241m*\u001b[39mtool_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtool_kwargs)\n\u001b[0;32m    414\u001b[0m )\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\tools.py:304\u001b[0m, in \u001b[0;36mBaseTool._parse_input\u001b[1;34m(self, tool_input)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43minput_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    306\u001b[0m             k: \u001b[38;5;28mgetattr\u001b[39m(result, k)\n\u001b[0;32m    307\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mdict()\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    308\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m tool_input\n\u001b[0;32m    309\u001b[0m         }\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tool_input\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\pydantic\\v1\\main.py:526\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[1;34m(cls, obj)\u001b[0m\n\u001b[0;32m    524\u001b[0m         exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected dict not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    525\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ValidationError([ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY)], \u001b[38;5;28mcls\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mobj)\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for websearchSchema\nquery\n  field required (type=value_error.missing)"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            if isinstance(value[\"messages\"][-1], BaseMessage):\n",
    "                print(\"Assistant:\", value[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
