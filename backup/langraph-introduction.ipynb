{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Advanced Conversational Agents Using Langgraph: A Comprehensive Guide\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In the realm of Natural Language Processing (NLP), conversational agents have become an integral part of various applications, from chatbots to virtual assistants. To build such advanced conversational agents, we need a robust framework that can handle complex dialogue management and multi-agent interactions. This is where Langgraph comes into play. Langgraph is a cutting-edge deep learning framework specifically designed for NLP applications, enabling developers to construct intricate Multi Agent Systems (MAS) with ease and efficiency. In this guide, we will delve into the world of Langgraph, exploring its features, setup, and best practices for building custom agents and optimizing performance.\n",
    "\n",
    "**Introduction to Langgraph**\n",
    "\n",
    "Langgraph is a Python-based framework that leverages the power of graph neural networks to model complex relationships between agents and their environments. This framework is built on top of the LangChain ecosystem, providing a seamless integration with other NLP tools and libraries.\n",
    "\n",
    "**Framework Overview**\n",
    "\n",
    "Langgraph's architecture is designed to handle large-scale conversational datasets, making it an ideal choice for building advanced conversational agents. The framework consists of three primary components:\n",
    "\n",
    "1. **Graph Construction**: Langgraph allows developers to create complex graphs that represent the relationships between agents, users, and environments.\n",
    "2. **Agent Modeling**: The framework provides a range of prebuils that can be customizet specific application requirements.\n",
    "3. **Dialogue Management**: Langgraph's dialogue management system enables agents to engage in contextual conversations, taking into account the graph structure and agent interactions.\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "Langgraph boasts several key features that set it apart from other NLP frameworks:\n",
    "\n",
    "* **Superior Performance & Efficiency**: Langgraph's graph-based architecture enables faster processing and more efficient memory usage, making it suitable for large-scale applications.\n",
    "* **Fully Integrated Into LangChain Ecosystem**: Langgraph is built on top of the LangChain ecosystem, providing seamless integration with other NLP tools and libraries.\n",
    "* **Active Research & Development Project**: Langgraph is an actively maintained project, with a dedicated community contributing to its development and growth.\n",
    "\n",
    "**Example in Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# Define the state of the graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], list]\n",
    "    sender: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "# Tool definition\n",
    "@tool\n",
    "def MyTool(text):\n",
    "    \"\"\"\"My Tool\n",
    "    \"\"\"\n",
    "    # Simulate processing time\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    # Generate a random response\n",
    "    import random\n",
    "    responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "    return random.choice(responses)\n",
    "# Create an agent\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant, collaborating with other assistants.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_tools(tools)\n",
    "\n",
    "# Define the nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    if isinstance(result, BaseMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = BaseMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response 3'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyTool.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from dotenv import load_dotenv\n",
    "# Environment setup\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_ibm import WatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WatsonxLLM initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 500,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the graph\n",
    "llm = ChatOpenAI(model=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import ( BaseMessage,)\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[dict, str, Literal[\"auto\", \"none\", \"any\", \"required\"], bool]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Assumes model is compatible with OpenAI tool-calling API.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                name of the tool (str): calls corresponding tool;\n",
    "                \"auto\": automatically selects a tool (including no tool);\n",
    "                \"none\": does not call a tool;\n",
    "                \"any\" or \"required\": force at least one tool to be called;\n",
    "                True: forces tool call (requires `tools` be length 1);\n",
    "                False: no effect;\n",
    "\n",
    "                or a dict of the form:\n",
    "                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "        if tool_choice:\n",
    "            if isinstance(tool_choice, str):\n",
    "                # tool_choice is a tool/function name\n",
    "                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n",
    "                    tool_choice = {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\"name\": tool_choice},\n",
    "                    }\n",
    "                # 'any' is not natively supported by OpenAI API.\n",
    "                # We support 'any' since other models use this instead of 'required'.\n",
    "                if tool_choice == \"any\":\n",
    "                    tool_choice = \"required\"\n",
    "            elif isinstance(tool_choice, bool):\n",
    "                if len(tools) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"tool_choice=True can only be used when a single tool is \"\n",
    "                        f\"passed in, received {len(tools)} tools.\"\n",
    "                    )\n",
    "                tool_choice = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": formatted_tools[0][\"function\"][\"name\"]},\n",
    "                }\n",
    "            elif isinstance(tool_choice, dict):\n",
    "                tool_names = [\n",
    "                    formatted_tool[\"function\"][\"name\"]\n",
    "                    for formatted_tool in formatted_tools\n",
    "                ]\n",
    "                if not any(\n",
    "                    tool_name == tool_choice[\"function\"][\"name\"]\n",
    "                    for tool_name in tool_names\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Tool choice {tool_choice} was specified, but the only \"\n",
    "                        f\"provided tools were {tool_names}.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n",
    "                    f\"Received: {tool_choice}\"\n",
    "                )\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        return super().bind(tools=formatted_tools, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WatsonxLLM.bind_tools() missing 1 required positional argument: 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWatsonxLLM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMyTool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou should provide a response.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mcreate_agent\u001b[1;34m(llm, tools, system_message)\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mpartial(system_message\u001b[38;5;241m=\u001b[39msystem_message)\n\u001b[0;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mpartial(tool_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]))\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt \u001b[38;5;241m|\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: WatsonxLLM.bind_tools() missing 1 required positional argument: 'tools'"
     ]
    }
   ],
   "source": [
    "agent = create_agent(WatsonxLLM,tools=[MyTool], system_message=\"You should provide a response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "logger = logging.getLogger(__name__)\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type: \"any\"},\n",
    "                        or {\"type: \"auto\"};\n",
    "            **kwargs: Any additional parameters to bind.\n",
    "\n",
    "        Example:\n",
    "            from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "            tool = TavilySearchResults(max_results=2)\n",
    "            tools = [tool]\n",
    "            llm_with_tools = WatsonxLLM.bind_tools(tools)\n",
    "        \"\"\"\n",
    "        formatted_tools = [self.convert_to_watson_tool(tool) for tool in tools]\n",
    "        if not tool_choice:\n",
    "            pass\n",
    "        elif isinstance(tool_choice, dict):\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n",
    "        elif isinstance(tool_choice, str):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n",
    "                f\"str, or None.\"\n",
    "            )\n",
    "        return self.bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    def convert_to_watson_tool(self, tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]) -> Dict[str, Any]:\n",
    "        \"\"\"Converts various tool types to a Watson-compatible format.\"\"\"\n",
    "        if isinstance(tool, BaseModel):\n",
    "            return tool.schema()\n",
    "        elif isinstance(tool, BaseTool):\n",
    "            return {\"name\": tool.name, \"description\": tool.description}  # or any other relevant fields\n",
    "        elif callable(tool):\n",
    "            return self.callable_to_dict(tool)\n",
    "        elif isinstance(tool, dict):\n",
    "            return tool\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tool type: {type(tool)}\")\n",
    "\n",
    "    def bind(self, tools: Sequence[Dict[str, Any]], **kwargs: Any) -> Runnable:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This should be implemented based on the specific needs of WatsonxLLM.\n",
    "        \"\"\"\n",
    "        # Example implementation, should be adapted to the actual WatsonxLLM needs\n",
    "        pass\n",
    "    def callable_to_dict(self, func: Callable) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a callable to a dictionary representation.\"\"\"\n",
    "        return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__,\n",
    "            \"parameters\": [],  # This should be adapted to extract parameters if needed\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    AIMessageChunk,\n",
    "    BaseMessage,\n",
    "    BaseMessageChunk,\n",
    "    ChatMessage,\n",
    "    ChatMessageChunk,\n",
    "    FunctionMessage,\n",
    "    FunctionMessageChunk,\n",
    "    HumanMessage,\n",
    "    HumanMessageChunk,\n",
    "    InvalidToolCall,\n",
    "    SystemMessage,\n",
    "    SystemMessageChunk,\n",
    "    ToolCall,\n",
    "    ToolMessage,\n",
    "    ToolMessageChunk,\n",
    ")\n",
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    "    PydanticOutputParser,\n",
    ")\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[dict, str, Literal[\"auto\", \"none\", \"any\", \"required\"], bool]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Assumes model is compatible with OpenAI tool-calling API.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                name of the tool (str): calls corresponding tool;\n",
    "                \"auto\": automatically selects a tool (including no tool);\n",
    "                \"none\": does not call a tool;\n",
    "                \"any\" or \"required\": force at least one tool to be called;\n",
    "                True: forces tool call (requires `tools` be length 1);\n",
    "                False: no effect;\n",
    "\n",
    "                or a dict of the form:\n",
    "                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "        if tool_choice:\n",
    "            if isinstance(tool_choice, str):\n",
    "                # tool_choice is a tool/function name\n",
    "                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n",
    "                    tool_choice = {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\"name\": tool_choice},\n",
    "                    }\n",
    "                # 'any' is not natively supported by OpenAI API.\n",
    "                # We support 'any' since other models use this instead of 'required'.\n",
    "                if tool_choice == \"any\":\n",
    "                    tool_choice = \"required\"\n",
    "            elif isinstance(tool_choice, bool):\n",
    "                if len(tools) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"tool_choice=True can only be used when a single tool is \"\n",
    "                        f\"passed in, received {len(tools)} tools.\"\n",
    "                    )\n",
    "                tool_choice = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": formatted_tools[0][\"function\"][\"name\"]},\n",
    "                }\n",
    "            elif isinstance(tool_choice, dict):\n",
    "                tool_names = [\n",
    "                    formatted_tool[\"function\"][\"name\"]\n",
    "                    for formatted_tool in formatted_tools\n",
    "                ]\n",
    "                if not any(\n",
    "                    tool_name == tool_choice[\"function\"][\"name\"]\n",
    "                    for tool_name in tool_names\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Tool choice {tool_choice} was specified, but the only \"\n",
    "                        f\"provided tools were {tool_names}.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n",
    "                    f\"Received: {tool_choice}\"\n",
    "                )\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        return super().bind(tools=formatted_tools, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for WatsonxLLM\n__root__\n  Did not find url, please add an environment variable `WATSONX_URL` which contains it, or pass `url` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 41\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [result],\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msender\u001b[39m\u001b[38;5;124m\"\u001b[39m: name,\n\u001b[0;32m     38\u001b[0m     }\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWatsonxLLM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMyTool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou should provide a response.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[41], line 25\u001b[0m, in \u001b[0;36mcreate_agent\u001b[1;34m(llm, tools, system_message)\u001b[0m\n\u001b[0;32m     23\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mpartial(system_message\u001b[38;5;241m=\u001b[39msystem_message)\n\u001b[0;32m     24\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mpartial(tool_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]))\n\u001b[1;32m---> 25\u001b[0m llm_instance \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt \u001b[38;5;241m|\u001b[39m llm_instance\u001b[38;5;241m.\u001b[39mbind_tools(tools)\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 1 validation error for WatsonxLLM\n__root__\n  Did not find url, please add an environment variable `WATSONX_URL` which contains it, or pass `url` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "# Tool definition\n",
    "@tool\n",
    "def MyTool(text):\n",
    "    \"\"\"My Tool\"\"\"\n",
    "    # Simulate processing time\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    # Generate a random response\n",
    "    import random\n",
    "    responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "    return random.choice(responses)\n",
    "\n",
    "# Create an agent\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant, collaborating with other assistants.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    llm_instance = llm()\n",
    "    return prompt | llm_instance.bind_tools(tools)\n",
    "\n",
    "# Define the nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    if isinstance(result, BaseMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = BaseMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "agent = create_agent(WatsonxLLM, tools=[MyTool], system_message=\"You should provide a response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WatsonxLLM.bind_tools() missing 1 required positional argument: 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWatsonxLLM\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMyTool\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou should provide a response.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[26], line 9\u001b[0m, in \u001b[0;36mcreate_agent\u001b[1;34m(llm, tools, system_message)\u001b[0m\n\u001b[0;32m      7\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mpartial(system_message\u001b[38;5;241m=\u001b[39msystem_message)\n\u001b[0;32m      8\u001b[0m prompt \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mpartial(tool_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([tool\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m tool \u001b[38;5;129;01min\u001b[39;00m tools]))\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt \u001b[38;5;241m|\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: WatsonxLLM.bind_tools() missing 1 required positional argument: 'tools'"
     ]
    }
   ],
   "source": [
    "\n",
    "agent = create_agent(WatsonxLLM,tools=[MyTool], system_message=\"You should provide a response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "agent_node_partial = functools.partial(agent_node, agent=agent, name=\"Agent\")\n",
    "\n",
    "tool_node = ToolNode([my_tool])\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Agent\", agent_node_partial)\n",
    "workflow.add_node(\"Tool\", tool_node)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Agent\",\n",
    "    lambda x: \"Tool\" if x[\"messages\"][-1].content == \"Use tool\" else \"Agent\",\n",
    "    {\"Agent\": \"Agent\", \"Tool\": \"Tool\"},\n",
    ")\n",
    "\n",
    "workflow.set_entry_point(\"Agent\")\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "# Invoke the graph\n",
    "events = graph.stream(\n",
    "    {\"messages\": [BaseMessage(content=\"Hello!\")]},\n",
    "    {\"recursion_limit\": 150},\n",
    ")\n",
    "\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langgraph' has no attribute 'Graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m random\u001b[38;5;241m.\u001b[39mchoice(responses)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize the graph\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mlanggraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m()\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Initialize the tool\u001b[39;00m\n\u001b[0;32m     21\u001b[0m my_tool \u001b[38;5;241m=\u001b[39m MyTool()\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'langgraph' has no attribute 'Graph'"
     ]
    }
   ],
   "source": [
    "import langgraph\n",
    "\n",
    "class MyTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"My Tool\"\n",
    "    \n",
    "    def process(self, text):\n",
    "        # Simulate processing time\n",
    "        import time\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Generate a random response\n",
    "        import random\n",
    "        responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "        return random.choice(responses)\n",
    "\n",
    "# Initialize the graph\n",
    "graph = langgraph.Graph()\n",
    "\n",
    "# Initialize the tool\n",
    "my_tool = MyTool()\n",
    "\n",
    "# Add nodes to the graph\n",
    "agent1 = langgraph.Agent('Agent 1')\n",
    "node1 = langgraph.Node('Node 1', my_tool.process)\n",
    "user = langgraph.User('User')\n",
    "\n",
    "# Connect the nodes\n",
    "graph.add_edge(agent1, node1, label='action')\n",
    "graph.add_edge(node1, user, label='response')\n",
    "\n",
    "# Set initial entry point\n",
    "graph.set_start(agent1)\n",
    "\n",
    "# Run the graph\n",
    "event = graph.run({'messages': [('user', 'Hello!')]})\n",
    "\n",
    "# Print the resulting message\n",
    "print(event['messages'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'langgraph' has no attribute 'Graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create a sample graph with two agents and a user\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mlanggraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m()\n\u001b[0;32m      5\u001b[0m agent1 \u001b[38;5;241m=\u001b[39m langgraph\u001b[38;5;241m.\u001b[39mAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent 1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m agent2 \u001b[38;5;241m=\u001b[39m langgraph\u001b[38;5;241m.\u001b[39mAgent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent 2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'langgraph' has no attribute 'Graph'"
     ]
    }
   ],
   "source": [
    "import langgraph\n",
    "\n",
    "# Create a sample graph with two agents and a user\n",
    "graph = langgraph.Graph()\n",
    "agent1 = langgraph.Agent(\"Agent 1\")\n",
    "agent2 = langgraph.Agent(\"Agent 2\")\n",
    "user = langgraph.User(\"User\")\n",
    "\n",
    "graph.add_node(agent1)\n",
    "graph.add_node(agent2)\n",
    "graph.add_node(user)\n",
    "\n",
    "graph.add_edge(agent1, user, \"speaks_to\")\n",
    "graph.add_edge(agent2, user, \"listens_to\")\n",
    "\n",
    "# Print the graph structure\n",
    "print(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langgraph.state_graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Annotated\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedDict\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_graph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StateGraph\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate_graph\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_messages\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyTool\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'langgraph.state_graph'"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "class MyTool:\n",
    "    def __init__(self):\n",
    "        self.name = \"My Tool\"\n",
    "    def process(self, text):\n",
    "        # Simulate processing time\n",
    "        import time\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Generate a random response\n",
    "        import random\n",
    "        responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "        return random.choice(responses)\n",
    "graph_builder = StateGraph(typed_dict=TypedDict, keys={'messages': Annotated[list, add_messages]})\n",
    "graph = graph_builder.build()\n",
    "# Initialize the tool\n",
    "my_tool = MyTool()\n",
    "# Add nodes to the graph\n",
    "agent1 = Agent('Agent 1')  # Fixed syntax error\n",
    "node1 = Node('Node 1', my_tool.process)\n",
    "user = User('User')\n",
    "# Connect the nodes\n",
    "graph.add_edge(agent1, node1, label='action')\n",
    "graph.add_edge(node1, user, label='response')\n",
    "# Set initial entry point\n",
    "graph.set_start(agent1)\n",
    "# Run the graph\n",
    "event = graph.run({'messages': [('user', 'Hello!')]})\n",
    "# Print the resulting message\n",
    "print(event['messages'][0]['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StateGraph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the graph\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlanggraph\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m graph_builder \u001b[38;5;241m=\u001b[39m \u001b[43mStateGraph\u001b[49m(typed_dict\u001b[38;5;241m=\u001b[39mTypedDict, keys\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: Annotated[\u001b[38;5;28mlist\u001b[39m, add_messages]})\n\u001b[0;32m      4\u001b[0m graph \u001b[38;5;241m=\u001b[39m graph_builder\u001b[38;5;241m.\u001b[39mbuild()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StateGraph' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (207578514.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    agent1 h.Agent('Agent 1')\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "**Setting Up Your Environment**\n",
    "\n",
    "Before diving into the world of Langgraph, let's set up our environment and configure our first graph.\n",
    "\n",
    "**Installing Dependencies**\n",
    "\n",
    "To get started with Langgraph, you'll need to install the required dependencies. You can do this using pip:\n",
    "```\n",
    "pip install langgraph\n",
    "```\n",
    "**Configuring Your First Graph**\n",
    "\n",
    "Once you've installed Langgraph, let's create a simple graph with two agents and a user:\n",
    "```python\n",
    "import langgraph\n",
    "\n",
    "# Create a sample graph with two agents and a user\n",
    "graph = langgraph.Graph()\n",
    "agent1 = langgraph.Agent(\"Agent 1\")\n",
    "agent2 = langgraph.Agent(\"Agent 2\")\n",
    "user = langgraph.User(\"User\")\n",
    "\n",
    "graph.add_node(agent1)\n",
    "graph.add_node(agent2)\n",
    "graph.add_node(user)\n",
    "\n",
    "graph.add_edge(agent1, user, \"speaks_to\")\n",
    "graph.add_edge(agent2, user, \"listens_to\")\n",
    "\n",
    "# Print the graph structure\n",
    "print(graph)\n",
    "```\n",
    "This code creates a simple graph with two agents and a user, where Agent 1 speaks to the user, and Agent 2 listens to the user.\n",
    "\n",
    "**Creating Your Own Custom Agents**\n",
    "\n",
    "Now that we have our environment set up, let's create our own custom agents using Langgraph.\n",
    "\n",
    "**Design Principles**\n",
    "\n",
    "When designing custom agents, it's essential to consider the following principles:\n",
    "\n",
    "* **Modularity**: Break down complex agent behaviors into smaller, reusable modules.\n",
    "* **Reusability**: Design agents that can be easily reused across different applications.\n",
    "* **Flexibility**: Create agents that can adapt to changing environments and user inputs.\n",
    "\n",
    "**Implementation Techniques**\n",
    "\n",
    "Langgraph provides several implementation techniques to create custom agents:\n",
    "\n",
    "* **Leveraginebuilt Tools**: Utilize Langgraph's prebuilt tools and libraries to speed up development.\n",
    "* **Extending Core Functionality**: Extend Langgraph's core functionality to create custom agents that meet specifin requirements.\n",
    "\n",
    "**Example in Python**\n",
    "```python\n",
    "import langgraph\n",
    "# Create a custom agent that greets users\n",
    "class GreetingAgent(langgraph.Agent):\n",
    "    def __init__(self, name):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def respond(self, user_input):\n",
    "        return f\"Hello, {user_input}!\"\n",
    "\n",
    "# Create an instance of the custom agent\n",
    "greeting_agent = GreetingAgent(\"Greeting Agent\")\n",
    "\n",
    "# Test the custom agent\n",
    "print(greeting_agent.respond(\"John\"))  # Output: Hello, John!\n",
    "```\n",
    "**Optimizing Performance Through Best Practices**\n",
    "\n",
    "To ensure optimal performance when building advanced conversational agents, follow these best practices:\n",
    "\n",
    "**Code Organization & Modularization**\n",
    "\n",
    "Organize your code into modular components, making it easier to maintain and update.\n",
    "\n",
    "**Utilizing Parallelism**\n",
    "\n",
    "Leverage parallel processing to speed up computationally intensive tasks, such as graph construction and agent modeling.\n",
    "\n",
    "**Memory Management Strategies**\n",
    "\n",
    "Implement efficient memory management strategies to reduce memory usage and prevent memory leaks.\n",
    "\n",
    "**Example in Python**\n",
    "```python\n",
    "import langgraph\n",
    "import concurrent.futures\n",
    "\n",
    "# Create a sample graph with multiple agents\n",
    "graph = langgraph.Graph()\n",
    "agents = [langgraph.Agent(f\"Agent {i}\") for i in range(10)]\n",
    "\n",
    "# Create a parallelized function to add agents to the graph\n",
    "def add_agents_to_graph(agents):\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(graph.add_node, agent) for agent in agents]\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "# Add agents to the graph in parallel\n",
    "add_agents_to_graph(agents)\n",
    "```\n",
    "**Troubleshooting Common Issues**\n",
    "\n",
    "When building advanced conversational agents, you may encounter common issues such as:\n",
    "\n",
    "* **Debugging Tips**: Use Langgraph's built-in debugging tools to identify and resolve issues.\n",
    "* **Community Support Resources**: Leverage the Langgraph community and online resources to troubleshoot common issues.\n",
    "\n",
    "By following this comprehensive guide, you'll be well on your way to building advanced conversational agents using Langgraph. Remember to explore the Langgraph documentation and community resources for further guidance and support."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
