{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[dict, str, Literal[\"auto\", \"none\", \"any\", \"required\"], bool]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Assumes model is compatible with OpenAI tool-calling API.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                name of the tool (str): calls corresponding tool;\n",
    "                \"auto\": automatically selects a tool (including no tool);\n",
    "                \"none\": does not call a tool;\n",
    "                \"any\" or \"required\": force at least one tool to be called;\n",
    "                True: forces tool call (requires `tools` be length 1);\n",
    "                False: no effect;\n",
    "\n",
    "                or a dict of the form:\n",
    "                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "        if tool_choice:\n",
    "            if isinstance(tool_choice, str):\n",
    "                # tool_choice is a tool/function name\n",
    "                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n",
    "                    tool_choice = {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\"name\": tool_choice},\n",
    "                    }\n",
    "                # 'any' is not natively supported by OpenAI API.\n",
    "                # We support 'any' since other models use this instead of 'required'.\n",
    "                if tool_choice == \"any\":\n",
    "                    tool_choice = \"required\"\n",
    "            elif isinstance(tool_choice, bool):\n",
    "                if len(tools) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"tool_choice=True can only be used when a single tool is \"\n",
    "                        f\"passed in, received {len(tools)} tools.\"\n",
    "                    )\n",
    "                tool_choice = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": formatted_tools[0][\"function\"][\"name\"]},\n",
    "                }\n",
    "            elif isinstance(tool_choice, dict):\n",
    "                tool_names = [\n",
    "                    formatted_tool[\"function\"][\"name\"]\n",
    "                    for formatted_tool in formatted_tools\n",
    "                ]\n",
    "                if not any(\n",
    "                    tool_name == tool_choice[\"function\"][\"name\"]\n",
    "                    for tool_name in tool_names\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Tool choice {tool_choice} was specified, but the only \"\n",
    "                        f\"provided tools were {tool_names}.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n",
    "                    f\"Received: {tool_choice}\"\n",
    "                )\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        return super().bind(tools=formatted_tools, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Sequence, Union\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "def summarize_search_results(results, max_results=4):\n",
    "  contents = [result[\"content\"] for result in results[:max_results]]\n",
    "  return \" \".join(contents)\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"Extended IBM watsonx.ai large language models.\"\"\"\n",
    "    tools: Sequence[Dict[str, Any]] = Field(default_factory=list)\n",
    "\n",
    "    def bind_and_invoke(self, tools: Sequence[BaseTool], prompt: str, **kwargs: Any) -> Any:\n",
    "        \"\"\"Method to actually bind the tools to the model and invoke it.\n",
    "           This method handles TavilySearchResults tools specifically.\"\"\"\n",
    "        self.tools = tools\n",
    "        # Process search results and update prompt\n",
    "        for tool in self.tools:\n",
    "            if isinstance(tool, TavilySearchResults):\n",
    "                search_results = tool.invoke(prompt)  # Get search results\n",
    "                # Update prompt based on search results (logic depends on your use case)\n",
    "                # Here's a simple example that concatenates top 2 results to the prompt\n",
    "                top_results = summarize_search_results(search_results)\n",
    "                new_prompt = f\" You are Chatbot Assitant and Answer the following query {prompt}. You can use this informaation from web search: {top_results}\"\n",
    "                break  # Only process the first TavilySearchResults tool\n",
    "        # Now you have the updated prompt\n",
    "        #print(\"prompt\", new_prompt)\n",
    "        return self.invoke(new_prompt, **kwargs)\n",
    "\n",
    "# Example usage\n",
    "parameters = {\n",
    "    \"GenParams.DECODING_METHOD\": \"SAMPLE\",\n",
    "    \"GenParams.MAX_NEW_TOKENS\": 500,\n",
    "    \"GenParams.MIN_NEW_TOKENS\": 1,\n",
    "    \"GenParams.TEMPERATURE\": 0.7,\n",
    "    \"GenParams.TOP_K\": 50,\n",
    "    \"GenParams.TOP_P\": 1,\n",
    "}\n",
    "model_id = \"meta-llama/llama-3-8b-instruct\" #\"ibm/granite-13b-instruct-v2\"\n",
    "llm = WatsonxLLM(model_id=model_id, url=url, apikey=api_key, project_id=project_id, params=parameters)\n",
    "tool = TavilySearchResults(max_results=4)\n",
    "prompt = \"What is the weather today in Genova\"\n",
    "result = llm.bind_and_invoke([tool], prompt)  # Bind tools to the model and invoke with the prompt\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Sequence, Union\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from pydantic import BaseModel, Field\n",
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"Extended IBM watsonx.ai large language models.\"\"\"\n",
    "    tools: Sequence[Dict[str, Any]] = Field(default_factory=list)\n",
    "    def bind(self, tools: Sequence[BaseTool], **kwargs: Any) -> Runnable:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This method handles TavilySearchResults tools specifically.\"\"\"\n",
    "        self.tools = tools\n",
    "        return self\n",
    "# Example usage\n",
    "parameters = {\n",
    "    \"GenParams.DECODING_METHOD\": \"SAMPLE\",\n",
    "    \"GenParams.MAX_NEW_TOKENS\": 100,\n",
    "    \"GenParams.MIN_NEW_TOKENS\": 1,\n",
    "    \"GenParams.TEMPERATURE\": 0.7,\n",
    "    \"GenParams.TOP_K\": 50,\n",
    "    \"GenParams.TOP_P\": 1,\n",
    "}\n",
    "model_id = \"meta-llama/llama-3-8b-instruct\"\n",
    "llm = WatsonxLLM(model_id=model_id, url=url, apikey=api_key, project_id=project_id, params=parameters)\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tool_message = {\"role\": \"assistant\", \"content\": tool.invoke(\"Who is Ruslan Magana?\")}\n",
    "print(\"tool_message\",tool_message)\n",
    "llm_with_tools = llm.bind([tool_message])  # Bind tools to the model\n",
    "result = llm_with_tools.invoke(\"Who is Ruslan Magana?\")  # Now invoke the model\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "logger = logging.getLogger(__name__)\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type: \"any\"},\n",
    "                        or {\"type: \"auto\"};\n",
    "            **kwargs: Any additional parameters to bind.\n",
    "\n",
    "        Example:\n",
    "            from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "            tool = TavilySearchResults(max_results=2)\n",
    "            tools = [tool]\n",
    "            llm_with_tools = WatsonxLLM.bind_tools(tools)\n",
    "        \"\"\"\n",
    "        formatted_tools = [self.convert_to_watson_tool(tool) for tool in tools]\n",
    "        if not tool_choice:\n",
    "            pass\n",
    "        elif isinstance(tool_choice, dict):\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n",
    "        elif isinstance(tool_choice, str):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n",
    "                f\"str, or None.\"\n",
    "            )\n",
    "        return self.bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    def convert_to_watson_tool(self, tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]) -> Dict[str, Any]:\n",
    "        \"\"\"Converts various tool types to a Watson-compatible format.\"\"\"\n",
    "        if isinstance(tool, BaseModel):\n",
    "            return tool.schema()\n",
    "        elif isinstance(tool, BaseTool):\n",
    "            return {\"name\": tool.name, \"description\": tool.description}  # or any other relevant fields\n",
    "        elif callable(tool):\n",
    "            return self.callable_to_dict(tool)\n",
    "        elif isinstance(tool, dict):\n",
    "            return tool\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tool type: {type(tool)}\")\n",
    "\n",
    "    def bind(self, tools: Sequence[Dict[str, Any]], **kwargs: Any) -> Runnable:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This should be implemented based on the specific needs of WatsonxLLM.\n",
    "        \"\"\"\n",
    "        # Example implementation, should be adapted to the actual WatsonxLLM needs\n",
    "        pass\n",
    "    def callable_to_dict(self, func: Callable) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a callable to a dictionary representation.\"\"\"\n",
    "        return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__,\n",
    "            \"parameters\": [],  # This should be adapted to extract parameters if needed\n",
    "        }\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    parameters = {\n",
    "        GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "        GenParams.MAX_NEW_TOKENS: 100,\n",
    "        GenParams.MIN_NEW_TOKENS: 1,\n",
    "        GenParams.TEMPERATURE: 0.7,\n",
    "        GenParams.TOP_K: 50,\n",
    "        GenParams.TOP_P: 1\n",
    "    }\n",
    "    model_id = \"meta-llama/llama-3-8b-instruct\"\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=url,\n",
    "        apikey=api_key,\n",
    "        project_id=project_id,\n",
    "        params=parameters\n",
    "    )\n",
    "    tool = TavilySearchResults(max_results=2)\n",
    "    tools = [tool]\n",
    "    llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def process_llm_result(llm_output):\n",
    "    def extract_numbers(arguments):\n",
    "        numbers = re.findall(r'\\d+', arguments)\n",
    "        if len(numbers) == 2:\n",
    "            return {\"first_int\": int(numbers[0]), \"second_int\": int(numbers[1])}\n",
    "        return None\n",
    "\n",
    "    if isinstance(llm_output, str):\n",
    "        try:\n",
    "            llm_output = json.loads(llm_output)\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    # Check if it's a valid dictionary with required keys\n",
    "    if isinstance(llm_output, dict):\n",
    "        if \"name\" in llm_output and \"arguments\" in llm_output:\n",
    "            if isinstance(llm_output[\"arguments\"], dict):\n",
    "                if \"first_int\" in llm_output[\"arguments\"] and \"second_int\" in llm_output[\"arguments\"]:\n",
    "                    return llm_output\n",
    "            elif isinstance(llm_output[\"arguments\"], (list, tuple)) and len(llm_output[\"arguments\"]) == 2:\n",
    "                try:\n",
    "                    return {\"name\": \"multiply\", \"arguments\": {\"first_int\": int(llm_output[\"arguments\"][0]), \"second_int\": int(llm_output[\"arguments\"][1])}}\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "    # List of regex patterns to match and fix the input strings\n",
    "    patterns = [\n",
    "        r'{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"(\\d+)\":\\s*\"(\\d+)\"}}',\n",
    "        r'name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)}',\n",
    "        r'name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)}\\}',\n",
    "        r'name:\\s*multiply,\\s*arguments:\\s*{(\\d+),\\s*(\\d+)}',\n",
    "        r'name:\\s*multiply,\\s*arguments:\\s*\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'name\\[multiply\\],\\s*arguments\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'name:\\s*multiply,\\s*arguments:\\s*{first_int:\\s*(\\d+),\\s*second_int:\\s*(\\d+)}',\n",
    "        r'name\\[multiply\\],\\s*arguments\\[(\\d+),(\\d+)\\]',\n",
    "        r'\\nname:\\s*multiply\\narguments:\\s*{first_int:\\s*(\\d+),\\s*second_int:\\s*(\\d+)}',\n",
    "        r'name:\\s*multiply\\narguments:\\s*{first_int:\\s*(\\d+),\\s*second_int:\\s*(\\d+)}',\n",
    "        r'name\\[JSON blob with \\'name\\' and \\'arguments\\' keys\\] argument\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'Human:\\s*{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)}}',\n",
    "        r'arguments\\{(\\d+),\\s*(\\d+)\\}name\\[multiply\\] arguments\\{(\\d+),\\s*(\\d+)\\}',\n",
    "        r',\"\\s*multiply\\((\\d+),\\s*(\\d+)\\)',\n",
    "        r' \\{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*\\[\"(\\d+)\",\\s*\"(\\d+)\"\\]\\}',\n",
    "        r'{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*\\[\"(\\d+)\",\\s*\"(\\d+)\"\\]}',\n",
    "        r'name\\[istani\\],\\s*arguments\\[(\\d+),\\s*(\\d+)\\]',\n",
    "        r'Response:\\s*multiply\\((\\d+),\\s*(\\d+)\\)',\n",
    "        r'Response:\\s*\\{\"name\":\\s*\"multiply\",\\s*\"arguments\":\\s*{\\s*\"first_int\":\\s*(\\d+),\\s*\"second_int\":\\s*(\\d+)\\s*}\\s*\\}'\n",
    "    ]\n",
    "\n",
    "    if isinstance(llm_output, str):\n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, llm_output)\n",
    "            if match:\n",
    "                numbers = match.groups()\n",
    "                if len(numbers) == 2:\n",
    "                    return {\"name\": \"multiply\", \"arguments\": {\"first_int\": int(numbers[0]), \"second_int\": int(numbers[1])}}\n",
    "\n",
    "    return {\"name\": \"multiply\", \"arguments\": {\"first_int\": \"\", \"second_int\": \"\"}}\n",
    "\n",
    "# Example usage\n",
    "test_cases = [\n",
    "    '{\"name\": \"multiply\", \"arguments\": {\"13\": \"4\"}}',\n",
    "    'name: multiply, arguments: {13, 4}',\n",
    "    \"name: multiply, arguments: ['13', '4']\",\n",
    "    'name[multiply], arguments[13, 4]',\n",
    "    ' returned {\"name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}}',\n",
    "    ',\"multiply(13, 4)',\n",
    "    'name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}',\n",
    "    'name: multiply, arguments: {first_int: 13, second_int: 4}',\n",
    "    'name[Multiply], arguments[13, 4]',\n",
    "    'name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}}',\n",
    "    'name[multiply], arguments[13,4]',\n",
    "    '{\"name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}}',\n",
    "    'name[istani], arguments[13, 4]',\n",
    "    'Mult',\n",
    "    '\\nname: multiply\\narguments: {first_int: 13, second_int: 4}',\n",
    "    ',\"multiply\"',\n",
    "    ' {\"name\": \"multiply\", \"arguments\": [\"13\", \"4\"]}',\n",
    "    '{\"name\": \"multiply\", \"arguments\": [\"13\", \"4\"]}',\n",
    "    'name: multiply\\narguments: {first_int: 13, second_int: 4}',\n",
    "    \"name[JSON blob with 'name' and 'arguments' keys] argument[13, 4]\",\n",
    "    'Human: {\"name\": \"multiply\", \"arguments\": {\"first_int\": 13, \"second_int\": 4}}',\n",
    "    ' arguments{13, 4}name[multiply] arguments{13, 4}',\n",
    "    'Response: multiply(13, 4)',\n",
    "    'Response:{\"name\": \"multiply\", \"arguments\": { \"first_int\": 13, \"second_int\": 4 } }'\n",
    "]\n",
    "\n",
    "#for case in test_cases:\n",
    "#    print(process_llm_result(case))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\"WatsonxLLM\" object has no field \"tools\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[135], line 108\u001b[0m\n",
      "\u001b[0;32m    106\u001b[0m tool \u001b[38;5;241m=\u001b[39m TavilySearchResults(max_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;32m    107\u001b[0m tools \u001b[38;5;241m=\u001b[39m [tool]\n",
      "\u001b[1;32m--> 108\u001b[0m llm_with_tools \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbind_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m    109\u001b[0m llm_with_tools\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather like in San Francisco?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[135], line 59\u001b[0m, in \u001b[0;36mWatsonxLLM.bind_tools\u001b[1;34m(self, tools, tool_choice, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool_choice\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m. Expected dict, str, or None.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbind(tools\u001b[38;5;241m=\u001b[39mformatted_tools, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\n",
      "Cell \u001b[1;32mIn[135], line 66\u001b[0m, in \u001b[0;36mWatsonxLLM.bind\u001b[1;34m(self, tools, **kwargs)\u001b[0m\n",
      "\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, tools: Sequence[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Runnable:\n",
      "\u001b[0;32m     62\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[0;32m     63\u001b[0m \u001b[38;5;124;03m    Method to actually bind the tools to the model.\u001b[39;00m\n",
      "\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    This should be implemented based on the specific needs of WatsonxLLM.\u001b[39;00m\n",
      "\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtools\u001b[49m \u001b[38;5;241m=\u001b[39m tools\n",
      "\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\pydantic\\v1\\main.py:357\u001b[0m, in \u001b[0;36mBaseModel.__setattr__\u001b[1;34m(self, name, value)\u001b[0m\n",
      "\u001b[0;32m    354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m object_setattr(\u001b[38;5;28mself\u001b[39m, name, value)\n",
      "\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mextra \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Extra\u001b[38;5;241m.\u001b[39mallow \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__fields__:\n",
      "\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m object has no field \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mallow_mutation \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__config__\u001b[38;5;241m.\u001b[39mfrozen:\n",
      "\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is immutable and does not support item assignment\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "\u001b[1;31mValueError\u001b[0m: \"WatsonxLLM\" object has no field \"tools\""
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "logger = logging.getLogger(__name__)\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type: \"any\"},\n",
    "                        or {\"type: \"auto\"};\n",
    "            **kwargs: Any additional parameters to bind.\n",
    "\n",
    "        Example:\n",
    "            from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "            tool = TavilySearchResults(max_results=2)\n",
    "            tools = [tool]\n",
    "            llm_with_tools = WatsonxLLM.bind_tools(tools)\n",
    "        \"\"\"\n",
    "        formatted_tools = [self.convert_to_watson_tool(tool) for tool in tools]\n",
    "        if not tool_choice:\n",
    "            pass\n",
    "        elif isinstance(tool_choice, dict):\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n",
    "        elif isinstance(tool_choice, str):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n",
    "                f\"str, or None.\"\n",
    "            )\n",
    "        return self.bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    def convert_to_watson_tool(self, tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]) -> Dict[str, Any]:\n",
    "        \"\"\"Converts various tool types to a Watson-compatible format.\"\"\"\n",
    "        if isinstance(tool, BaseModel):\n",
    "            return tool.schema()\n",
    "        elif isinstance(tool, BaseTool):\n",
    "            return {\"name\": tool.name, \"description\": tool.description}  # or any other relevant fields\n",
    "        elif callable(tool):\n",
    "            return self.callable_to_dict(tool)\n",
    "        elif isinstance(tool, dict):\n",
    "            return tool\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tool type: {type(tool)}\")\n",
    "\n",
    "    def bind(self, tools: Sequence[Dict[str, Any]], **kwargs: Any) -> Runnable:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This should be implemented based on the specific needs of WatsonxLLM.\n",
    "        \"\"\"\n",
    "        # Example implementation, should be adapted to the actual WatsonxLLM needs\n",
    "        pass\n",
    "    def callable_to_dict(self, func: Callable) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a callable to a dictionary representation.\"\"\"\n",
    "        return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__,\n",
    "            \"parameters\": [],  # This should be adapted to extract parameters if needed\n",
    "        }\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    parameters = {\n",
    "        GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "        GenParams.MAX_NEW_TOKENS: 100,\n",
    "        GenParams.MIN_NEW_TOKENS: 1,\n",
    "        GenParams.TEMPERATURE: 0.7,\n",
    "        GenParams.TOP_K: 50,\n",
    "        GenParams.TOP_P: 1\n",
    "    }\n",
    "    model_id = \"meta-llama/llama-3-8b-instruct\"\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=url,\n",
    "        apikey=api_key,\n",
    "        project_id=project_id,\n",
    "        params=parameters\n",
    "    )\n",
    "    tool = TavilySearchResults(max_results=2)\n",
    "    tools = [tool]\n",
    "    llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import ( Runnable)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type: \"any\"},\n",
    "                        or {\"type: \"auto\"};\n",
    "            **kwargs: Any additional parameters to bind.\n",
    "\n",
    "        Example:\n",
    "            from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "            tool = TavilySearchResults(max_results=2)\n",
    "            tools = [tool]\n",
    "            llm_with_tools = WatsonxLLM.bind_tools(tools)\n",
    "        \"\"\"\n",
    "        formatted_tools = [self.convert_to_watson_tool(tool) for tool in tools]\n",
    "        if not tool_choice:\n",
    "            pass\n",
    "        elif isinstance(tool_choice, dict):\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n",
    "        elif isinstance(tool_choice, str):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n",
    "                f\"str, or None.\"\n",
    "            )\n",
    "        return self.bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    def convert_to_watson_tool(self, tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]) -> Dict[str, Any]:\n",
    "        \"\"\"Converts various tool types to a Watson-compatible format.\"\"\"\n",
    "        if isinstance(tool, BaseModel):\n",
    "            return tool.schema()\n",
    "        elif isinstance(tool, BaseTool):\n",
    "            return tool.to_dict()\n",
    "        elif callable(tool):\n",
    "            return self.callable_to_dict(tool)\n",
    "        elif isinstance(tool, dict):\n",
    "            return tool\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tool type: {type(tool)}\")\n",
    "\n",
    "    def bind(self, tools: Sequence[Dict[str, Any]], **kwargs: Any) -> Runnable:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This should be implemented based on the specific needs of WatsonxLLM.\n",
    "        \"\"\"\n",
    "        # Example implementation, should be adapted to the actual WatsonxLLM needs\n",
    "        pass\n",
    "\n",
    "    def callable_to_dict(self, func: Callable) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a callable to a dictionary representation.\"\"\"\n",
    "        # Example implementation\n",
    "        return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__,\n",
    "            \"parameters\": [],  # This should be adapted to extract parameters if needed\n",
    "        }\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    parameters = {\n",
    "        GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "        GenParams.MAX_NEW_TOKENS: 100,\n",
    "        GenParams.MIN_NEW_TOKENS: 1,\n",
    "        GenParams.TEMPERATURE: 0.7,\n",
    "        GenParams.TOP_K: 50,\n",
    "        GenParams.TOP_P: 1\n",
    "    }\n",
    "    model_id = \"meta-llama/llama-3-8b-instruct\"\n",
    "    llm = WatsonxLLM(\n",
    "        model_id=model_id,\n",
    "        url=url,\n",
    "        apikey=api_key,\n",
    "        project_id=project_id,\n",
    "        params=parameters\n",
    "    )\n",
    "    tool = TavilySearchResults(max_results=2)\n",
    "    tools = [tool]\n",
    "    # Modification: tell the LLM which tools it can call\n",
    "    llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[Dict[str, str], str]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Any:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type: \"any\"},\n",
    "                        or {\"type\": \"auto\"};\n",
    "            **kwargs: Any additional parameters to bind.\n",
    "\n",
    "        Example:\n",
    "            from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "            tool = TavilySearchResults(max_results=2)\n",
    "            tools = [tool]\n",
    "            llm_with_tools = WatsonxLLM.bind_tools(tools)\n",
    "        \"\"\"\n",
    "        formatted_tools = [self.convert_to_watson_tool(tool) for tool in tools]\n",
    "        if not tool_choice:\n",
    "            pass\n",
    "        elif isinstance(tool_choice, dict):\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n",
    "        elif isinstance(tool_choice, str):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n",
    "                f\"str, or None.\"\n",
    "            )\n",
    "        return self.bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    def convert_to_watson_tool(self, tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]) -> Dict[str, Any]:\n",
    "        \"\"\"Converts various tool types to a Watson-compatible format.\"\"\"\n",
    "        if isinstance(tool, BaseModel):\n",
    "            return tool.schema()\n",
    "        elif isinstance(tool, BaseTool):\n",
    "            return tool.to_dict()\n",
    "        elif callable(tool):\n",
    "            return self.callable_to_dict(tool)\n",
    "        elif isinstance(tool, dict):\n",
    "            return tool\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tool type: {type(tool)}\")\n",
    "\n",
    "    def bind(self, tools: Sequence[Dict[str, Any]], **kwargs: Any) -> Any:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This should be implemented based on the specific needs of WatsonxLLM.\n",
    "        \"\"\"\n",
    "        # Example implementation, should be adapted to the actual WatsonxLLM needs\n",
    "        logger.info(\"Binding tools to WatsonxLLM model.\")\n",
    "        # Replace the below line with actual binding logic\n",
    "        return tools\n",
    "\n",
    "    def callable_to_dict(self, func: Callable) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a callable to a dictionary representation.\"\"\"\n",
    "        # Example implementation\n",
    "        return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__,\n",
    "            \"parameters\": [],  # This should be adapted to extract parameters if needed\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "    \n",
    "    tool = TavilySearchResults(max_results=2)\n",
    "    tools = [tool]\n",
    "    \n",
    "    llm = WatsonxLLM()\n",
    "    llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "from typing import Sequence, Union, Dict, Any, Optional, Callable, Type\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.tools import BaseTool\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Literal,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "class WatsonxLLM:\n",
    "    # Assuming other parts of WatsonxLLM class are defined elsewhere\n",
    "\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[Dict[str, str], Literal[\"any\", \"auto\"], str]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                    name of the tool (str): calls corresponding tool;\n",
    "                    \"auto\" or None: automatically selects a tool (including no tool);\n",
    "                    \"any\": force at least one tool to be called;\n",
    "                    or a dict of the form:\n",
    "                        {\"type\": \"tool\", \"name\": \"tool_name\"},\n",
    "                        or {\"type: \"any\"},\n",
    "                        or {\"type: \"auto\"};\n",
    "            **kwargs: Any additional parameters to bind.\n",
    "\n",
    "        Example:\n",
    "            from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "            tool = TavilySearchResults(max_results=2)\n",
    "            tools = [tool]\n",
    "            llm_with_tools = WatsonxLLM.bind_tools(tools)\n",
    "        \"\"\"\n",
    "        formatted_tools = [self.convert_to_watson_tool(tool) for tool in tools]\n",
    "        if not tool_choice:\n",
    "            pass\n",
    "        elif isinstance(tool_choice, dict):\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        elif isinstance(tool_choice, str) and tool_choice in (\"any\", \"auto\"):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": tool_choice}\n",
    "        elif isinstance(tool_choice, str):\n",
    "            kwargs[\"tool_choice\"] = {\"type\": \"tool\", \"name\": tool_choice}\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unrecognized 'tool_choice' type {tool_choice=}. Expected dict, \"\n",
    "                f\"str, or None.\"\n",
    "            )\n",
    "        return self.bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "    def convert_to_watson_tool(self, tool: Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]) -> Dict[str, Any]:\n",
    "        \"\"\"Converts various tool types to a Watson-compatible format.\"\"\"\n",
    "        if isinstance(tool, BaseModel):\n",
    "            return tool.schema()\n",
    "        elif isinstance(tool, BaseTool):\n",
    "            return tool.to_dict()\n",
    "        elif callable(tool):\n",
    "            return self.callable_to_dict(tool)\n",
    "        elif isinstance(tool, dict):\n",
    "            return tool\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported tool type: {type(tool)}\")\n",
    "    def bind(self, tools: Sequence[Dict[str, Any]], **kwargs: Any) -> Runnable:\n",
    "        \"\"\"Method to actually bind the tools to the model.\n",
    "        This should be implemented based on the specific needs of WatsonxLLM.\n",
    "        \"\"\"\n",
    "        # Example implementation, should be adapted to the actual WatsonxLLM needs\n",
    "        pass\n",
    "\n",
    "    def callable_to_dict(self, func: Callable) -> Dict[str, Any]:\n",
    "        \"\"\"Convert a callable to a dictionary representation.\"\"\"\n",
    "        # Example implementation\n",
    "        return {\n",
    "            \"name\": func.__name__,\n",
    "            \"description\": func.__doc__,\n",
    "            \"parameters\": [],  # This should be adapted to extract parameters if needed\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Any, Dict, Iterator, List, Mapping, Optional, Union\n",
    "\n",
    "from ibm_watsonx_ai import Credentials  # type: ignore\n",
    "from ibm_watsonx_ai.foundation_models import Model, ModelInference  # type: ignore\n",
    "from langchain_core.callbacks import CallbackManagerForLLMRun\n",
    "from langchain_core.language_models.llms import BaseLLM\n",
    "from langchain_core.outputs import Generation, GenerationChunk, LLMResult\n",
    "from langchain_core.pydantic_v1 import Extra, Field, SecretStr, root_validator\n",
    "from langchain_core.utils import convert_to_secret_str, get_from_dict_or_env\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class WatsonxLLM(BaseLLM):\n",
    "    \"\"\"\n",
    "    IBM watsonx.ai large language models.\n",
    "\n",
    "    To use, you should have ``langchain_ibm`` python package installed,\n",
    "    and the environment variable ``WATSONX_APIKEY`` set with your API key, or pass\n",
    "    it as a named parameter to the constructor.\n",
    "\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            from ibm_watsonx_ai.metanames import GenTextParamsMetaNames\n",
    "            parameters = {\n",
    "                GenTextParamsMetaNames.DECODING_METHOD: \"sample\",\n",
    "                GenTextParamsMetaNames.MAX_NEW_TOKENS: 100,\n",
    "                GenTextParamsMetaNames.MIN_NEW_TOKENS: 1,\n",
    "                GenTextParamsMetaNames.TEMPERATURE: 0.5,\n",
    "                GenTextParamsMetaNames.TOP_K: 50,\n",
    "                GenTextParamsMetaNames.TOP_P: 1,\n",
    "            }\n",
    "\n",
    "            from langchain_ibm import WatsonxLLM\n",
    "            watsonx_llm = WatsonxLLM(\n",
    "                model_id=\"google/flan-ul2\",\n",
    "                url=\"https://us-south.ml.cloud.ibm.com\",\n",
    "                apikey=\"*****\",\n",
    "                project_id=\"*****\",\n",
    "                params=parameters,\n",
    "            )\n",
    "    \"\"\"\n",
    "\n",
    "    model_id: str = \"\"\n",
    "    \"\"\"Type of model to use.\"\"\"\n",
    "\n",
    "    deployment_id: str = \"\"\n",
    "    \"\"\"Type of deployed model to use.\"\"\"\n",
    "\n",
    "    project_id: str = \"\"\n",
    "    \"\"\"ID of the Watson Studio project.\"\"\"\n",
    "\n",
    "    space_id: str = \"\"\n",
    "    \"\"\"ID of the Watson Studio space.\"\"\"\n",
    "\n",
    "    url: Optional[SecretStr] = None\n",
    "    \"\"\"Url to Watson Machine Learning or CPD instance\"\"\"\n",
    "\n",
    "    apikey: Optional[SecretStr] = None\n",
    "    \"\"\"Apikey to Watson Machine Learning or CPD instance\"\"\"\n",
    "\n",
    "    token: Optional[SecretStr] = None\n",
    "    \"\"\"Token to CPD instance\"\"\"\n",
    "\n",
    "    password: Optional[SecretStr] = None\n",
    "    \"\"\"Password to CPD instance\"\"\"\n",
    "\n",
    "    username: Optional[SecretStr] = None\n",
    "    \"\"\"Username to CPD instance\"\"\"\n",
    "\n",
    "    instance_id: Optional[SecretStr] = None\n",
    "    \"\"\"Instance_id of CPD instance\"\"\"\n",
    "\n",
    "    version: Optional[SecretStr] = None\n",
    "    \"\"\"Version of CPD instance\"\"\"\n",
    "\n",
    "    params: Optional[dict] = None\n",
    "    \"\"\"Model parameters to use during generate requests.\"\"\"\n",
    "\n",
    "    verify: Union[str, bool, None] = None\n",
    "    \"\"\"User can pass as verify one of following:\n",
    "        the path to a CA_BUNDLE file\n",
    "        the path of directory with certificates of trusted CAs\n",
    "        True - default path to truststore will be taken\n",
    "        False - no verification will be made\"\"\"\n",
    "\n",
    "    streaming: bool = False\n",
    "    \"\"\" Whether to stream the results or not. \"\"\"\n",
    "\n",
    "    watsonx_model: ModelInference = Field(default=None, exclude=True)  #: :meta private:\n",
    "\n",
    "    class Config:\n",
    "        \"\"\"Configuration for this pydantic object.\"\"\"\n",
    "\n",
    "        extra = Extra.forbid\n",
    "\n",
    "    @classmethod\n",
    "    def is_lc_serializable(cls) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def lc_secrets(self) -> Dict[str, str]:\n",
    "        \"\"\"A map of constructor argument names to secret ids.\n",
    "\n",
    "        For example:\n",
    "            {\n",
    "                \"url\": \"WATSONX_URL\",\n",
    "                \"apikey\": \"WATSONX_APIKEY\",\n",
    "                \"token\": \"WATSONX_TOKEN\",\n",
    "                \"password\": \"WATSONX_PASSWORD\",\n",
    "                \"username\": \"WATSONX_USERNAME\",\n",
    "                \"instance_id\": \"WATSONX_INSTANCE_ID\",\n",
    "            }\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"url\": \"WATSONX_URL\",\n",
    "            \"apikey\": \"WATSONX_APIKEY\",\n",
    "            \"token\": \"WATSONX_TOKEN\",\n",
    "            \"password\": \"WATSONX_PASSWORD\",\n",
    "            \"username\": \"WATSONX_USERNAME\",\n",
    "            \"instance_id\": \"WATSONX_INSTANCE_ID\",\n",
    "        }\n",
    "\n",
    "    @root_validator()\n",
    "    def validate_environment(cls, values: Dict) -> Dict:\n",
    "        \"\"\"Validate that credentials and python package exists in environment.\"\"\"\n",
    "        if isinstance(values.get(\"watsonx_model\"), (ModelInference, Model)):\n",
    "            values[\"model_id\"] = getattr(values[\"watsonx_model\"], \"model_id\")\n",
    "            values[\"deployment_id\"] = getattr(\n",
    "                values[\"watsonx_model\"], \"deployment_id\", \"\"\n",
    "            )\n",
    "            values[\"project_id\"] = getattr(\n",
    "                getattr(values[\"watsonx_model\"], \"_client\"),\n",
    "                \"default_project_id\",\n",
    "            )\n",
    "            values[\"space_id\"] = getattr(\n",
    "                getattr(values[\"watsonx_model\"], \"_client\"), \"default_space_id\"\n",
    "            )\n",
    "            values[\"params\"] = getattr(values[\"watsonx_model\"], \"params\")\n",
    "        else:\n",
    "            values[\"url\"] = convert_to_secret_str(\n",
    "                get_from_dict_or_env(values, \"url\", \"WATSONX_URL\")\n",
    "            )\n",
    "            if \"cloud.ibm.com\" in values.get(\"url\", \"\").get_secret_value():\n",
    "                values[\"apikey\"] = convert_to_secret_str(\n",
    "                    get_from_dict_or_env(values, \"apikey\", \"WATSONX_APIKEY\")\n",
    "                )\n",
    "            else:\n",
    "                if (\n",
    "                    not values[\"token\"]\n",
    "                    and \"WATSONX_TOKEN\" not in os.environ\n",
    "                    and not values[\"password\"]\n",
    "                    and \"WATSONX_PASSWORD\" not in os.environ\n",
    "                    and not values[\"apikey\"]\n",
    "                    and \"WATSONX_APIKEY\" not in os.environ\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        \"Did not find 'token', 'password' or 'apikey',\"\n",
    "                        \" please add an environment variable\"\n",
    "                        \" `WATSONX_TOKEN`, 'WATSONX_PASSWORD' or 'WATSONX_APIKEY' \"\n",
    "                        \"which contains it,\"\n",
    "                        \" or pass 'token', 'password' or 'apikey'\"\n",
    "                        \" as a named parameter.\"\n",
    "                    )\n",
    "                elif values[\"token\"] or \"WATSONX_TOKEN\" in os.environ:\n",
    "                    values[\"token\"] = convert_to_secret_str(\n",
    "                        get_from_dict_or_env(values, \"token\", \"WATSONX_TOKEN\")\n",
    "                    )\n",
    "                elif values[\"password\"] or \"WATSONX_PASSWORD\" in os.environ:\n",
    "                    values[\"password\"] = convert_to_secret_str(\n",
    "                        get_from_dict_or_env(values, \"password\", \"WATSONX_PASSWORD\")\n",
    "                    )\n",
    "                    values[\"username\"] = convert_to_secret_str(\n",
    "                        get_from_dict_or_env(values, \"username\", \"WATSONX_USERNAME\")\n",
    "                    )\n",
    "                elif values[\"apikey\"] or \"WATSONX_APIKEY\" in os.environ:\n",
    "                    values[\"apikey\"] = convert_to_secret_str(\n",
    "                        get_from_dict_or_env(values, \"apikey\", \"WATSONX_APIKEY\")\n",
    "                    )\n",
    "                    values[\"username\"] = convert_to_secret_str(\n",
    "                        get_from_dict_or_env(values, \"username\", \"WATSONX_USERNAME\")\n",
    "                    )\n",
    "                if not values[\"instance_id\"] or \"WATSONX_INSTANCE_ID\" not in os.environ:\n",
    "                    values[\"instance_id\"] = convert_to_secret_str(\n",
    "                        get_from_dict_or_env(\n",
    "                            values, \"instance_id\", \"WATSONX_INSTANCE_ID\"\n",
    "                        )\n",
    "                    )\n",
    "            credentials = Credentials(\n",
    "                url=values[\"url\"].get_secret_value() if values[\"url\"] else None,\n",
    "                api_key=values[\"apikey\"].get_secret_value()\n",
    "                if values[\"apikey\"]\n",
    "                else None,\n",
    "                token=values[\"token\"].get_secret_value() if values[\"token\"] else None,\n",
    "                password=values[\"password\"].get_secret_value()\n",
    "                if values[\"password\"]\n",
    "                else None,\n",
    "                username=values[\"username\"].get_secret_value()\n",
    "                if values[\"username\"]\n",
    "                else None,\n",
    "                instance_id=values[\"instance_id\"].get_secret_value()\n",
    "                if values[\"instance_id\"]\n",
    "                else None,\n",
    "                version=values[\"version\"].get_secret_value()\n",
    "                if values[\"version\"]\n",
    "                else None,\n",
    "                verify=values[\"verify\"],\n",
    "            )\n",
    "\n",
    "            watsonx_model = ModelInference(\n",
    "                model_id=values[\"model_id\"],\n",
    "                deployment_id=values[\"deployment_id\"],\n",
    "                credentials=credentials,\n",
    "                params=values[\"params\"],\n",
    "                project_id=values[\"project_id\"],\n",
    "                space_id=values[\"space_id\"],\n",
    "            )\n",
    "            values[\"watsonx_model\"] = watsonx_model\n",
    "\n",
    "        return values\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\n",
    "            \"model_id\": self.model_id,\n",
    "            \"deployment_id\": self.deployment_id,\n",
    "            \"params\": self.params,\n",
    "            \"project_id\": self.project_id,\n",
    "            \"space_id\": self.space_id,\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Return type of llm.\"\"\"\n",
    "        return \"IBM watsonx.ai\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_token_usage(\n",
    "        response: Optional[List[Dict[str, Any]]] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        if response is None:\n",
    "            return {\"generated_token_count\": 0, \"input_token_count\": 0}\n",
    "\n",
    "        input_token_count = 0\n",
    "        generated_token_count = 0\n",
    "\n",
    "        def get_count_value(key: str, result: Dict[str, Any]) -> int:\n",
    "            return result.get(key, 0) or 0\n",
    "\n",
    "        for res in response:\n",
    "            results = res.get(\"results\")\n",
    "            if results:\n",
    "                input_token_count += get_count_value(\"input_token_count\", results[0])\n",
    "                generated_token_count += get_count_value(\n",
    "                    \"generated_token_count\", results[0]\n",
    "                )\n",
    "\n",
    "        return {\n",
    "            \"generated_token_count\": generated_token_count,\n",
    "            \"input_token_count\": input_token_count,\n",
    "        }\n",
    "\n",
    "    def _get_chat_params(\n",
    "        self, stop: Optional[List[str]] = None\n",
    "    ) -> Optional[Dict[str, Any]]:\n",
    "        params: Optional[Dict[str, Any]] = {**self.params} if self.params else None\n",
    "        if stop is not None:\n",
    "            params = (params or {}) | {\"stop_sequences\": stop}\n",
    "        return params\n",
    "\n",
    "    def _create_llm_result(self, response: List[dict]) -> LLMResult:\n",
    "        \"\"\"Create the LLMResult from the choices and prompts.\"\"\"\n",
    "        generations = []\n",
    "        for res in response:\n",
    "            results = res.get(\"results\")\n",
    "            if results:\n",
    "                finish_reason = results[0].get(\"stop_reason\")\n",
    "                gen = Generation(\n",
    "                    text=results[0].get(\"generated_text\"),\n",
    "                    generation_info={\"finish_reason\": finish_reason},\n",
    "                )\n",
    "                generations.append([gen])\n",
    "        final_token_usage = self._extract_token_usage(response)\n",
    "        llm_output = {\n",
    "            \"token_usage\": final_token_usage,\n",
    "            \"model_id\": self.model_id,\n",
    "            \"deployment_id\": self.deployment_id,\n",
    "        }\n",
    "        return LLMResult(generations=generations, llm_output=llm_output)\n",
    "\n",
    "    def _stream_response_to_generation_chunk(\n",
    "        self,\n",
    "        stream_response: Dict[str, Any],\n",
    "    ) -> GenerationChunk:\n",
    "        \"\"\"Convert a stream response to a generation chunk.\"\"\"\n",
    "        if not stream_response[\"results\"]:\n",
    "            return GenerationChunk(text=\"\")\n",
    "        return GenerationChunk(\n",
    "            text=stream_response[\"results\"][0][\"generated_text\"],\n",
    "            generation_info=dict(\n",
    "                finish_reason=stream_response[\"results\"][0].get(\"stop_reason\", None),\n",
    "                llm_output={\n",
    "                    \"model_id\": self.model_id,\n",
    "                    \"deployment_id\": self.deployment_id,\n",
    "                },\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> str:\n",
    "        \"\"\"Call the IBM watsonx.ai inference endpoint.\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "            run_manager: Optional callback manager.\n",
    "        Returns:\n",
    "            The string generated by the model.\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = watsonx_llm.invoke(\"What is a molecule\")\n",
    "        \"\"\"\n",
    "        result = self._generate(\n",
    "            prompts=[prompt], stop=stop, run_manager=run_manager, **kwargs\n",
    "        )\n",
    "        return result.generations[0][0].text\n",
    "\n",
    "    def _generate(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        stream: Optional[bool] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> LLMResult:\n",
    "        \"\"\"Call the IBM watsonx.ai inference endpoint which then generate the response.\n",
    "        Args:\n",
    "            prompts: List of strings (prompts) to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "            run_manager: Optional callback manager.\n",
    "        Returns:\n",
    "            The full LLMResult output.\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = watsonx_llm.generate([\"What is a molecule\"])\n",
    "        \"\"\"\n",
    "        params = self._get_chat_params(stop=stop)\n",
    "        should_stream = stream if stream is not None else self.streaming\n",
    "        if should_stream:\n",
    "            if len(prompts) > 1:\n",
    "                raise ValueError(\n",
    "                    f\"WatsonxLLM currently only supports single prompt, got {prompts}\"\n",
    "                )\n",
    "            generation = GenerationChunk(text=\"\")\n",
    "            stream_iter = self._stream(\n",
    "                prompts[0], stop=stop, run_manager=run_manager, **kwargs\n",
    "            )\n",
    "            for chunk in stream_iter:\n",
    "                if generation is None:\n",
    "                    generation = chunk\n",
    "                else:\n",
    "                    generation += chunk\n",
    "            assert generation is not None\n",
    "            if isinstance(generation.generation_info, dict):\n",
    "                llm_output = generation.generation_info.pop(\"llm_output\")\n",
    "                return LLMResult(generations=[[generation]], llm_output=llm_output)\n",
    "            return LLMResult(generations=[[generation]])\n",
    "        else:\n",
    "            response = self.watsonx_model.generate(\n",
    "                prompt=prompts, params=params, **kwargs\n",
    "            )\n",
    "            return self._create_llm_result(response)\n",
    "\n",
    "    def _stream(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Iterator[GenerationChunk]:\n",
    "        \"\"\"Call the IBM watsonx.ai inference endpoint which then streams the response.\n",
    "        Args:\n",
    "            prompt: The prompt to pass into the model.\n",
    "            stop: Optional list of stop words to use when generating.\n",
    "            run_manager: Optional callback manager.\n",
    "        Returns:\n",
    "            The iterator which yields generation chunks.\n",
    "        Example:\n",
    "            .. code-block:: python\n",
    "\n",
    "                response = watsonx_llm.stream(\"What is a molecule\")\n",
    "                for chunk in response:\n",
    "                    print(chunk, end='')\n",
    "        \"\"\"\n",
    "        params = self._get_chat_params(stop=stop)\n",
    "        for stream_resp in self.watsonx_model.generate_text_stream(\n",
    "            prompt=prompt, raw_response=True, params=params, **kwargs\n",
    "        ):\n",
    "            if not isinstance(stream_resp, dict):\n",
    "                stream_resp = stream_resp.dict()\n",
    "            chunk = self._stream_response_to_generation_chunk(stream_resp)\n",
    "\n",
    "            if run_manager:\n",
    "                run_manager.on_llm_new_token(chunk.text, chunk=chunk)\n",
    "            yield chunk\n",
    "\n",
    "    def get_num_tokens(self, text: str) -> int:\n",
    "        response = self.watsonx_model.tokenize(text, return_tokens=False)\n",
    "        return response[\"result\"][\"token_count\"]\n",
    "\n",
    "    def get_token_ids(self, text: str) -> List[int]:\n",
    "        raise NotImplementedError(\"API does not support returning token ids.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
