{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (ToolMessage)\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from typing import (\n",
    "    Any,\n",
    "    AsyncIterator,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Mapping,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    TypedDict,\n",
    "    TypeVar,\n",
    "    Union,\n",
    "    cast,\n",
    "    overload,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool definition\n",
    "@tool\n",
    "def MyTool(text):\n",
    "    \"\"\"My Tool\"\"\"\n",
    "    # Simulate processing time\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    # Generate a random response\n",
    "    import random\n",
    "    responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "    return random.choice(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response 3'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyTool.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=[MyTool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://langchain-ai.github.io/langgraph/concepts/',\n",
       "  'content': \"If a node has multiple out-going edges, all of those destination nodes will be executed in parallel as a part of the next superstep. State ManagementÂ¶ LangGraph introduces two key ideas to state management: state schemas and reducers. The state schema defines the type of the object that is given to each of the graph's Node.\"},\n",
       " {'url': 'https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141',\n",
       "  'content': 'Nodes: Nodes are the building blocks of your LangGraph. Each node represents a function or a computation step. You define nodes to perform specific tasks, such as processing input, making ...'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "tool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_tools = [convert_to_openai_tool(tool)[\"function\"] for tool in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'description': 'search query to look up',\n",
       "     'type': 'string'}},\n",
       "   'required': ['query']}}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'tavily_search_results_json',\n",
       "  'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'description': 'search query to look up',\n",
       "     'type': 'string'}},\n",
       "   'required': ['query']}}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_dict=formatted_tools[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_kwargs = {}\n",
    "if \"name\" in _dict:\n",
    "    additional_kwargs[\"name\"] = _dict[\"name\"]\n",
    "name = _dict.get(\"name\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.', additional_kwargs={'name': 'tavily_search_results_json'}, name='tavily_search_results_json', tool_call_id='1')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ToolMessage(\n",
    "            #content=_dict.get(\"content\", \"\"),\n",
    "            content=_dict.get(\"description\", \"\"),\n",
    "            tool_call_id=1, #cast(str, _dict.get(\"tool_call_id\")),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            name=name,\n",
    "            #id=id_,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ToolMessage(\n",
    "            content=_dict.get(\"content\", \"\"),\n",
    "            #content=_dict.get(\"description\", \"\"),\n",
    "            tool_call_id=1, #cast(str, _dict.get(\"tool_call_id\")),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            name=name,\n",
    "            #id=id_,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_info ={\"finish_reason\": \"Invocation Tool\"} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ChatGeneration(\n",
    "    message=message,\n",
    "    generation_info=generation_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "generations.append(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_output = {\n",
    "    \"token_usage\": 0,\n",
    "    \"model_name\": \"test\",\n",
    "    \"system_fingerprint\": \"test\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResult(generations=[ChatGeneration(generation_info={'finish_reason': 'Invocation Tool'}, message=ToolMessage(content='', additional_kwargs={'name': 'tavily_search_results_json'}, name='tavily_search_results_json', tool_call_id='1'))], llm_output={'token_usage': 0, 'model_name': 'test', 'system_fingerprint': 'test'})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatResult(generations=generations, llm_output=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_result=ChatResult(generations=generations, llm_output=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the generated text\n",
    "for generation in chat_result.generations:\n",
    "    print(generation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_usage': 0, 'model_name': 'test', 'system_fingerprint': 'test'}\n"
     ]
    }
   ],
   "source": [
    "# Print the raw LLM output (if you need to inspect it)\n",
    "print(chat_result.llm_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_dict_to_message(_dict: Mapping[str, Any]) -> BaseMessage:\n",
    "    \"\"\"Convert a dictionary to a LangChain message.\n",
    "\n",
    "    Args:\n",
    "        _dict: The dictionary.\n",
    "\n",
    "    Returns:\n",
    "        The LangChain message.\n",
    "    \"\"\"\n",
    "    role = _dict.get(\"role\")\n",
    "    name = _dict.get(\"name\")\n",
    "    id_ = _dict.get(\"id\")\n",
    "    if role == \"user\":\n",
    "        return HumanMessage(content=_dict.get(\"content\", \"\"), id=id_, name=name)\n",
    "    elif role == \"assistant\":\n",
    "        # Fix for azure\n",
    "        # Also OpenAI returns None for tool invocations\n",
    "        content = _dict.get(\"content\", \"\") or \"\"\n",
    "        additional_kwargs: Dict = {}\n",
    "        if function_call := _dict.get(\"function_call\"):\n",
    "            additional_kwargs[\"function_call\"] = dict(function_call)\n",
    "        tool_calls = []\n",
    "        invalid_tool_calls = []\n",
    "        if raw_tool_calls := _dict.get(\"tool_calls\"):\n",
    "            additional_kwargs[\"tool_calls\"] = raw_tool_calls\n",
    "            for raw_tool_call in raw_tool_calls:\n",
    "                try:\n",
    "                    tool_calls.append(parse_tool_call(raw_tool_call, return_id=True))\n",
    "                except Exception as e:\n",
    "                    invalid_tool_calls.append(\n",
    "                        make_invalid_tool_call(raw_tool_call, str(e))\n",
    "                    )\n",
    "        return AIMessage(\n",
    "            content=content,\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            name=name,\n",
    "            id=id_,\n",
    "            tool_calls=tool_calls,\n",
    "            invalid_tool_calls=invalid_tool_calls,\n",
    "        )\n",
    "    elif role == \"system\":\n",
    "        return SystemMessage(content=_dict.get(\"content\", \"\"), name=name, id=id_)\n",
    "    elif role == \"function\":\n",
    "        return FunctionMessage(\n",
    "            content=_dict.get(\"content\", \"\"), name=cast(str, _dict.get(\"name\")), id=id_\n",
    "        )\n",
    "    elif role == \"tool\":\n",
    "        additional_kwargs = {}\n",
    "        if \"name\" in _dict:\n",
    "            additional_kwargs[\"name\"] = _dict[\"name\"]\n",
    "        return ToolMessage(\n",
    "            content=_dict.get(\"content\", \"\"),\n",
    "            tool_call_id=cast(str, _dict.get(\"tool_call_id\")),\n",
    "            additional_kwargs=additional_kwargs,\n",
    "            name=name,\n",
    "            id=id_,\n",
    "        )\n",
    "    else:\n",
    "        return ChatMessage(content=_dict.get(\"content\", \"\"), role=role, id=id_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'token_usage': 0, 'model_name': 'test', 'system_fingerprint': 'test'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from typing import Any, Dict\n",
    "\n",
    "def generate_tool_output(tool_instance, input_text: str, max_results: int = 2)-> ChatResult:\n",
    "    \"\"\"\n",
    "    Generate chatbot output using the given tool and input text.\n",
    "    Args:\n",
    "    - tool_instance\n",
    "    - input_text (str): Input text to process\n",
    "    Returns:\n",
    "    - ChatResult: The generated chatbot output\n",
    "    \"\"\"\n",
    "    # Invoke the tool with the input text\n",
    "    tool_instance.invoke(input_text)\n",
    "    # Convert the tool to an OpenAI tool\n",
    "    formatted_tool = convert_to_openai_tool(tool_instance)[\"function\"]\n",
    "    # Create a ToolMessage instance\n",
    "    message = ToolMessage(\n",
    "        content=formatted_tool.get(\"content\", \"\"),\n",
    "        tool_call_id=1,\n",
    "        additional_kwargs={\"name\": formatted_tool.get(\"name\")},\n",
    "        name=formatted_tool.get(\"name\"),\n",
    "    )\n",
    "    # Create a ChatGeneration instance\n",
    "    generation_info = {\"finish_reason\": \"Invocation Tool\"}\n",
    "    gen = ChatGeneration(message=message, generation_info=generation_info)\n",
    "    # Create a list of generations\n",
    "    generations = [gen]\n",
    "    # Create an LLM output instance\n",
    "    llm_output = {\n",
    "        \"token_usage\": 0,\n",
    "        \"model_name\": \"test\",\n",
    "        \"system_fingerprint\": \"test\",\n",
    "    }\n",
    "    # Create a ChatResult instance\n",
    "    chat_result = ChatResult(generations=generations, llm_output=llm_output)\n",
    "    return chat_result\n",
    "# Example usage:\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "input_text = \"What's a 'node' in LangGraph?\"\n",
    "chat_result = generate_tool_output(tool, input_text)\n",
    "# Print the generated text\n",
    "for generation in chat_result.generations:\n",
    "    print(generation.text)\n",
    "# Print the raw LLM output (if you need to inspect it)\n",
    "print(chat_result.llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL: https://medium.com/@cplog/introduction-to-langgraph-a-beginners-guide-14f9be027141\n",
      "Content: Step 1: Define the Graph State. First, we define the state structure for our graph. In this example, our state includes the user's question, the classification of the question, and a response ...\n",
      "\n",
      "URL: https://langchain-ai.github.io/langgraph/concepts/\n",
      "Content: LangGraph's underlying graph algorithm uses message passing to define a general program. When a Node completes, it sends a message along one or more edges to other node (s). These nodes run their functions, pass the resulting messages to the next set of nodes, and on and on it goes. Inspired by Pregel, the program proceeds in discrete \"super-steps\" that are all executed conceptually in ...\n",
      "{'token_usage': 0, 'model_name': 'test', 'system_fingerprint': 'test'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import ChatGeneration, ChatResult\n",
    "from typing import Any, Dict\n",
    "\n",
    "def generate_tool_output(tool_instance: BaseTool, input_text: str, max_results: int = 2) -> ChatResult:\n",
    "    \"\"\"\n",
    "    Generate chatbot output using the given tool and input text.\n",
    "    Args:\n",
    "    - tool_instance: The instance of the tool to use.\n",
    "    - input_text (str): Input text to process.\n",
    "    Returns:\n",
    "    - ChatResult: The generated chatbot output.\n",
    "    \"\"\"\n",
    "    # Invoke the tool with the input text and capture the result\n",
    "    result = tool_instance.invoke(input_text)\n",
    "\n",
    "    # Convert the tool to an OpenAI tool\n",
    "    formatted_tool = convert_to_openai_tool(tool_instance)[\"function\"]\n",
    "\n",
    "    # Extract the content from the result to include in the ToolMessage\n",
    "    content = \"\\n\\n\".join([f\"URL: {item['url']}\\nContent: {item['content']}\" for item in result])\n",
    "\n",
    "    # Create a ToolMessage instance\n",
    "    message = ToolMessage(\n",
    "        content=content,\n",
    "        tool_call_id=1,\n",
    "        additional_kwargs={\"name\": formatted_tool.get(\"name\")},\n",
    "        name=formatted_tool.get(\"name\"),\n",
    "    )\n",
    "\n",
    "    # Create a ChatGeneration instance\n",
    "    generation_info = {\"finish_reason\": \"Invocation Tool\"}\n",
    "    gen = ChatGeneration(message=message, generation_info=generation_info)\n",
    "\n",
    "    # Create a list of generations\n",
    "    generations = [gen]\n",
    "\n",
    "    # Create an LLM output instance\n",
    "    llm_output = {\n",
    "        \"token_usage\": 0,\n",
    "        \"model_name\": \"test\",\n",
    "        \"system_fingerprint\": \"test\",\n",
    "    }\n",
    "\n",
    "    # Create a ChatResult instance\n",
    "    chat_result = ChatResult(generations=generations, llm_output=llm_output)\n",
    "\n",
    "    return chat_result\n",
    "\n",
    "# Example usage:\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "input_text = \"What's a 'node' in LangGraph?\"\n",
    "chat_result = generate_tool_output(tool, input_text)\n",
    "\n",
    "# Print the generated text\n",
    "for generation in chat_result.generations:\n",
    "    print(generation.message.content)\n",
    "\n",
    "# Print the raw LLM output (if you need to inspect it)\n",
    "print(chat_result.llm_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting and Evaluating Tools in OpenAI Format\n",
    "\n",
    "Converting tools to the OpenAI format and evaluating them is a useful process when you want to leverage the standardized API for integrating tools with OpenAI models. This is necessary for compatibility and to ensure that tools can be easily called by the model during interaction. The `convert_to_openai_tool` function translates the tool into a format that is recognized by the OpenAI API, allowing it to be utilized effectively within the context of language model interactions.\n",
    "\n",
    "### Steps for Conversion and Evaluation\n",
    "\n",
    "1. **Convert the Tool:**\n",
    "   The `convert_to_openai_tool` function transforms the tool into a standardized dictionary format that OpenAI's API can understand. This includes the tool's name, description, and parameters.\n",
    "\n",
    "2. **Bind the Tool:**\n",
    "   Use the `bind_tools` method to attach these tools to the chat model. This prepares the tools to be invoked by the model as needed.\n",
    "\n",
    "3. **Invoke the Tool:**\n",
    "   To evaluate and test the tool within the OpenAI format, you need to simulate a call to the tool as if the model is using it.\n",
    "\n",
    "Let's walk through an example using your provided code.\n",
    "\n",
    "### Example\n",
    "\n",
    "```python\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "\n",
    "# Step 1: Create and convert the tool\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "converted_tool = convert_to_openai_tool(tool)\n",
    "\n",
    "# Step 2: Format the tools for OpenAI\n",
    "formatted_tools = [converted_tool[\"function\"]]\n",
    "\n",
    "# Example of formatted_tools\n",
    "# formatted_tools = [{\n",
    "#     'name': 'tavily_search_results_json',\n",
    "#     'description': 'A search engine optimized for comprehensive, accurate, and trusted results. Useful for when you need to answer questions about current events. Input should be a search query.',\n",
    "#     'parameters': {'type': 'object', 'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query']}\n",
    "# }]\n",
    "\n",
    "# Step 3: Define a mock chat model (simplified example for illustration)\n",
    "class MockChatModel:\n",
    "    def __init__(self, tools):\n",
    "        self.tools = {tool['name']: tool for tool in tools}\n",
    "\n",
    "    def call_tool(self, tool_name, params):\n",
    "        if tool_name not in self.tools:\n",
    "            raise ValueError(f\"Tool {tool_name} not found\")\n",
    "        \n",
    "        # Simulate tool invocation\n",
    "        if tool_name == 'tavily_search_results_json':\n",
    "            # Here you call the original tool's method with params\n",
    "            query = params['query']\n",
    "            return tool.invoke(query)  # Original tool invocation\n",
    "        else:\n",
    "            raise ValueError(f\"Tool {tool_name} not implemented\")\n",
    "\n",
    "# Step 4: Instantiate the chat model with the converted tools\n",
    "chat_model = MockChatModel(formatted_tools)\n",
    "\n",
    "# Step 5: Invoke the tool through the chat model\n",
    "query = \"What's a 'node' in LangGraph?\"\n",
    "response = chat_model.call_tool('tavily_search_results_json', {'query': query})\n",
    "print(response)\n",
    "\n",
    "```\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Conversion:**\n",
    "   - The `TavilySearchResults` tool is instantiated and then converted to the OpenAI format using `convert_to_openai_tool`.\n",
    "\n",
    "2. **Formatting:**\n",
    "   - The formatted tool dictionary is prepared for binding to the chat model.\n",
    "\n",
    "3. **Mock Chat Model:**\n",
    "   - A simplified `MockChatModel` is created to simulate tool invocation.\n",
    "   - This model has a method `call_tool` that takes the tool name and parameters, finds the tool, and calls the original tool's `invoke`\n",
    "\n",
    " method with the given parameters.\n",
    "\n",
    "4. **Invocation:**\n",
    "   - The tool is invoked by calling `call_tool` on the mock chat model, which simulates how the tool would be used within a language model interaction.\n",
    "\n",
    "### Why Convert to OpenAI Tool Format?\n",
    "\n",
    "1. **Standardization:**\n",
    "   - The OpenAI tool format provides a standardized way to define and use tools, ensuring compatibility and ease of integration with language models.\n",
    "\n",
    "2. **Interoperability:**\n",
    "   - By converting tools to this format, they can be easily shared, reused, and combined with other tools in a consistent manner.\n",
    "\n",
    "3. **Ease of Use:**\n",
    "   - Tools in the OpenAI format can be invoked programmatically within conversations, enabling seamless interaction and functionality.\n",
    "\n",
    "### How to Use the Converted Tools\n",
    "\n",
    "Once the tools are converted and bound to the chat model, you can evaluate their functionality by invoking them as shown in the example. This approach allows you to test the tool's behavior in response to specific inputs and ensure that it performs as expected within the OpenAI tool-calling framework.\n",
    "\n",
    "By following these steps, you can effectively convert, bind, and evaluate tools within the OpenAI ecosystem, leveraging their standardized API for powerful and flexible interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
