{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.messages import (BaseMessage,)\n",
    "from langchain_core.output_parsers import ( JsonOutputParser,PydanticOutputParser,)\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "import json\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_ibm import WatsonxLLM\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# Environment setup\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "# Tool definition\n",
    "@tool\n",
    "def MyTool(text):\n",
    "    \"\"\"\"My Tool\n",
    "    \"\"\"\n",
    "    # Simulate processing time\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    # Generate a random response\n",
    "    import random\n",
    "    responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "    return random.choice(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response 3'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyTool.invoke(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Any, Dict, Sequence, Union, Type, Optional, Callable, Literal\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.tools import BaseTool, tool\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_ibm import WatsonxLLM as BaseWatsonxLLM\n",
    "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, SecretStr, root_validator\n",
    "from langchain_core.runnables import Runnable, RunnableMap, RunnablePassthrough\n",
    "from langchain_core.messages import (BaseMessage, ToolMessage,)\n",
    "from langchain.output_parsers.openai_tools import PydanticToolsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.language_models import LanguageModelInput\n",
    "from langchain_core.language_models.chat_models import BaseChatModel\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class WatsonxLLM(BaseWatsonxLLM):\n",
    "    \"\"\"\n",
    "    Extended IBM watsonx.ai large language models.\n",
    "    \"\"\"\n",
    "    def bind_tools(\n",
    "        self,\n",
    "        tools: Sequence[Union[Dict[str, Any], Type[BaseModel], Callable, BaseTool]],\n",
    "        *,\n",
    "        tool_choice: Optional[\n",
    "            Union[dict, str, Literal[\"auto\", \"none\", \"any\", \"required\"], bool]\n",
    "        ] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> Runnable[LanguageModelInput, BaseMessage]:\n",
    "        \"\"\"Bind tool-like objects to this chat model.\n",
    "\n",
    "        Assumes model is compatible with OpenAI tool-calling API.\n",
    "\n",
    "        Args:\n",
    "            tools: A list of tool definitions to bind to this chat model.\n",
    "                Can be  a dictionary, pydantic model, callable, or BaseTool. Pydantic\n",
    "                models, callables, and BaseTools will be automatically converted to\n",
    "                their schema dictionary representation.\n",
    "            tool_choice: Which tool to require the model to call.\n",
    "                Options are:\n",
    "                name of the tool (str): calls corresponding tool;\n",
    "                \"auto\": automatically selects a tool (including no tool);\n",
    "                \"none\": does not call a tool;\n",
    "                \"any\" or \"required\": force at least one tool to be called;\n",
    "                True: forces tool call (requires `tools` be length 1);\n",
    "                False: no effect;\n",
    "\n",
    "                or a dict of the form:\n",
    "                {\"type\": \"function\", \"function\": {\"name\": <<tool_name>>}}.\n",
    "            **kwargs: Any additional parameters to pass to the\n",
    "                :class:`~langchain.runnable.Runnable` constructor.\n",
    "        \"\"\"\n",
    "\n",
    "        formatted_tools = [convert_to_openai_tool(tool) for tool in tools]\n",
    "        if tool_choice:\n",
    "            if isinstance(tool_choice, str):\n",
    "                # tool_choice is a tool/function name\n",
    "                if tool_choice not in (\"auto\", \"none\", \"any\", \"required\"):\n",
    "                    tool_choice = {\n",
    "                        \"type\": \"function\",\n",
    "                        \"function\": {\"name\": tool_choice},\n",
    "                    }\n",
    "                # 'any' is not natively supported by OpenAI API.\n",
    "                # We support 'any' since other models use this instead of 'required'.\n",
    "                if tool_choice == \"any\":\n",
    "                    tool_choice = \"required\"\n",
    "            elif isinstance(tool_choice, bool):\n",
    "                if len(tools) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"tool_choice=True can only be used when a single tool is \"\n",
    "                        f\"passed in, received {len(tools)} tools.\"\n",
    "                    )\n",
    "                tool_choice = {\n",
    "                    \"type\": \"function\",\n",
    "                    \"function\": {\"name\": formatted_tools[0][\"function\"][\"name\"]},\n",
    "                }\n",
    "            elif isinstance(tool_choice, dict):\n",
    "                tool_names = [\n",
    "                    formatted_tool[\"function\"][\"name\"]\n",
    "                    for formatted_tool in formatted_tools\n",
    "                ]\n",
    "                if not any(\n",
    "                    tool_name == tool_choice[\"function\"][\"name\"]\n",
    "                    for tool_name in tool_names\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Tool choice {tool_choice} was specified, but the only \"\n",
    "                        f\"provided tools were {tool_names}.\"\n",
    "                    )\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    f\"Unrecognized tool_choice type. Expected str, bool or dict. \"\n",
    "                    f\"Received: {tool_choice}\"\n",
    "                )\n",
    "            kwargs[\"tool_choice\"] = tool_choice\n",
    "        return super().bind(tools=formatted_tools, **kwargs)\n",
    "\n",
    "# Environment setup\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"    \n",
    "# Create an instance of WatsonxLLM\n",
    "# WatsonxLLM initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 500,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_instance  = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")\n",
    "\n",
    "# Tool definition\n",
    "@tool\n",
    "def MyTool(text):\n",
    "    \"\"\"My Tool\"\"\"\n",
    "    # Simulate processing time\n",
    "    import time\n",
    "    time.sleep(1)\n",
    "    # Generate a random response\n",
    "    import random\n",
    "    responses = ['Response 1', 'Response 2', 'Response 3']\n",
    "    return random.choice(responses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Bind the tool to the instance\n",
    "llm_with_tools = watsonx_instance.bind_tools([MyTool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModelInference.generate() got an unexpected keyword argument 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is Ruslan Magana?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4433\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   4428\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4429\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   4430\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   4432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   4434\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   4436\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   4437\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_ibm\\llms.py:380\u001b[0m, in \u001b[0;36mWatsonxLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]])\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwatsonx_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    381\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompts, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_llm_result(response)\n",
      "\u001b[1;31mTypeError\u001b[0m: ModelInference.generate() got an unexpected keyword argument 'tools'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "llm_with_tools.invoke(\"Who is Ruslan Magana?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ev?Ruslan Maganaev is a Russian professional ice hockey right wing currently playing for Sibir Novosibirsk in the Vysshaya Liga, the second level of ice hockey in Russia. He previously played for Traktor Chelyabinsk in the Vysshaya Liga from 2009–10 to 2011–12.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "watsonx_instance.invoke(\"Who is Ruslan Magana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "# Create an agent\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant, collaborating with other assistants.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    llm_instance = llm()\n",
    "    return prompt | llm_instance.bind_tools(tools)\n",
    "# Example usage\n",
    "agent = create_agent(watsonx_instance, tools=[MyTool], system_message=\"You should provide a response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "# Create an agent\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful AI assistant, collaborating with other assistants.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ])\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    llm_instance = llm  # Pass the prompt to the llm instance\n",
    "    return llm_instance.bind_tools(tools)\n",
    "\n",
    "# Example usage\n",
    "agent = create_agent(watsonx_instance, tools=[MyTool], system_message=\"You should provide a response.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.agents import AgentExecutor, AgentType, initialize_agent, load_tools\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "# Since chains can be stateful (e.g. they can have memory), we provide\n",
    "# a way to initialize a new chain for each row in the dataset. This is done\n",
    "# by passing in a factory function that returns a new chain for each row.\n",
    "def create_agent(prompt, llm_with_tools):\n",
    "    runnable_agent = (\n",
    "        {\n",
    "            \"input\": lambda x: x[\"input\"],\n",
    "            \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "                x[\"intermediate_steps\"]\n",
    "            ),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm_with_tools\n",
    "        | OpenAIToolsAgentOutputParser()\n",
    "    )\n",
    "    return AgentExecutor(agent=runnable_agent, tools=tools, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_openai_tools_agent\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "def create_agent(llm: ChatOpenAI, tools: list, system_prompt: str):\n",
    "    # Each worker node will be given a name and some tools.\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                system_prompt,\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "        ]\n",
    "    )\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "agent = create_agent(watsonx_instance, tools=[MyTool], system_prompt=\"You should provide a response.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    if isinstance(result, BaseMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = BaseMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"sender\": name,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langgraph.graph import END, StateGraph\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from typing_extensions import TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the nodes\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    if isinstance(result, BaseMessage):\n",
    "        pass\n",
    "    else:\n",
    "        result = BaseMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    return {\n",
    "        \"messages\": [result],\n",
    "        \"sender\": name,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = BasicToolNode(tools=[MyTool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm =watsonx_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [MyTool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modification: tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ModelInference.generate() got an unexpected keyword argument 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mllm_with_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWho is Ruslan Magana?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\runnables\\base.py:4433\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   4427\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   4428\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4429\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   4430\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   4431\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   4432\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbound\u001b[38;5;241m.\u001b[39minvoke(\n\u001b[0;32m   4434\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   4435\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_configs(config),\n\u001b[0;32m   4436\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs},\n\u001b[0;32m   4437\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 276\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    277\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    278\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    279\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    280\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    281\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    282\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    283\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    284\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    285\u001b[0m         )\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    288\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         )\n\u001b[0;32m    802\u001b[0m     ]\n\u001b[1;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    804\u001b[0m         prompts, stop, run_managers, \u001b[38;5;28mbool\u001b[39m(new_arg_supported), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    805\u001b[0m     )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_core\\language_models\\llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 657\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    658\u001b[0m                 prompts,\n\u001b[0;32m    659\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    660\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    661\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    662\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    663\u001b[0m             )\n\u001b[0;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    666\u001b[0m         )\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langchain_ibm\\llms.py:380\u001b[0m, in \u001b[0;36mWatsonxLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]])\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwatsonx_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    381\u001b[0m         prompt\u001b[38;5;241m=\u001b[39mprompts, params\u001b[38;5;241m=\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    382\u001b[0m     )\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_llm_result(response)\n",
      "\u001b[1;31mTypeError\u001b[0m: ModelInference.generate() got an unexpected keyword argument 'tools'"
     ]
    }
   ],
   "source": [
    "llm_with_tools.invoke(\"Who is Ruslan Magana?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first argument is the unique node name\n",
    "# The second argument is the function or object that will be called whenever\n",
    "# the node is used.\n",
    "graph_builder.add_node(\"chatbot\", chatbot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Response 3'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyTool.invoke(\"What's a 'node' in LangGraph?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_node = BasicToolNode(tools=[MyTool])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding a node to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
     ]
    }
   ],
   "source": [
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "\n",
    "def route_tools(\n",
    "    state: State,\n",
    ") -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n",
    "\n",
    "    has tool calls. Otherwise, route to the end.\"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'node' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m graph_builder\u001b[38;5;241m.\u001b[39madd_edge(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m graph_builder\u001b[38;5;241m.\u001b[39mset_entry_point(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchatbot\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mgraph_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langgraph\\graph\\state.py:226\u001b[0m, in \u001b[0;36mStateGraph.compile\u001b[1;34m(self, checkpointer, interrupt_before, interrupt_after, debug)\u001b[0m\n\u001b[0;32m    223\u001b[0m interrupt_after \u001b[38;5;241m=\u001b[39m interrupt_after \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# validate the graph\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# prepare output channels\u001b[39;00m\n\u001b[0;32m    235\u001b[0m state_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannels)\n",
      "File \u001b[1;32mc:\\Dropbox\\23-GITHUB\\Projects\\How-to-build-Multi-Agent-System-with-Langraph\\.venv\\lib\\site-packages\\langgraph\\graph\\graph.py:295\u001b[0m, in \u001b[0;36mGraph.validate\u001b[1;34m(self, interrupt)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a dead-end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m all_sources:\n\u001b[1;32m--> 295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mnode\u001b[49m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes \u001b[38;5;129;01mand\u001b[39;00m source \u001b[38;5;241m!=\u001b[39m START:\n\u001b[0;32m    296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound edge starting at unknown node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;66;03m# assemble targets\u001b[39;00m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'node' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"__end__\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", \"__end__\": \"__end__\"},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.set_entry_point(\"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            if isinstance(value[\"messages\"][-1], BaseMessage):\n",
    "                print(\"Assistant:\", value[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
