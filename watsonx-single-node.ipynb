{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Agent System with Langraph with WatsonX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello everyone today we are going to learn how o build a Single Agent System with Langraph with WatsonX\n",
    "We'll start with a basic chatbot introducing key LangGraph concepts along the way.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langsmith\n",
    "\n",
    "# Used for this tutorial; not a requirement for LangGraph\n",
    "%pip install -U langchain_anthropic\n",
    "\n",
    "# Used for install IBM WatsonX\n",
    "%pip install python-dotenv  ibm_watson_machine_learning\n",
    "%pip install \"ibm-watsonx-ai\" \n",
    "%pip install \"pydantic>=1.10.0\" \n",
    "%pip install \"langchain>=0.1.52\"\n",
    "%pip install \"langchain_ibm>=0.1.7\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we setup our API keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "def _set_env(var: str):\n",
    "    load_dotenv()  # Load environment variables from .env file\n",
    "    env_var = os.getenv(var)\n",
    "    if not env_var:\n",
    "        env_var = getpass.getpass(f\"{var}: \")\n",
    "        os.environ[var] = env_var\n",
    "    return env_var\n",
    "\n",
    "api_key = _set_env(\"WATSONX_API_KEY\")\n",
    "project_id = _set_env(\"PROJECT_ID\")\n",
    "url = \"https://us-south.ml.cloud.ibm.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Encouraged) [LangSmith](https://smith.langchain.com/) makes it a lot easier to see what's going on \"under the hood.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_set_env(\"LANGSMITH_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"LangGraph Tutorial\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Basic Chatbot with WatsonX and LangGraph\n",
    "Let's create a simple chatbot using LangGraph with WatsonX. This chatbot will respond directly to user messages.\n",
    "\n",
    "**What is LangGraph?**\n",
    "LangGraph is a powerful tool that helps us build complex systems, like chatbots, by connecting different pieces of code together. Think of it like a flowchart, where each step builds on the previous one.\n",
    "\n",
    "**Creating a StateGraph**\n",
    "We'll start by creating a `StateGraph`. A `StateGraph` object defines the structure of our chatbot as a \"state machine\". This means we can think of our chatbot as a series of states, and we'll define how it transitions between these states.\n",
    "\n",
    "**Adding Nodes and Edges**\n",
    "We'll add `nodes` to represent the different functions our chatbot can perform, like responding to user messages. We'll also add `edges` to specify how the bot should transition between these functions.\n",
    "\n",
    "Here's the code that sets up the LangGraph State:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the Code**\n",
    "Let's break down what's happening in this code:\n",
    "\n",
    "* We've defined our `State` as a special type of dictionary called a `TypedDict`. This dictionary has a single key: `messages`.\n",
    "* The `messages` key is special because we're telling LangGraph how to update it. We're using a function called `add_messages` to say, \"Hey, when we add new messages, append them to the existing list instead of overwriting it.\"\n",
    "* Now our graph knows two important things:\n",
    "\t1. Every `node` we define will receive the current `State` as input and return a value that updates that state.\n",
    "\t2. `messages` will be appended to the current list, rather than directly overwritten.\n",
    "\n",
    "**Next Steps**\n",
    "Next, we'll add a \"chatbot\" node. Nodes represent individual tasks or functions, and they're typically regular Python functions. We'll explore this in more detail in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Foundation Models on WatsonX.ai**\n",
    "\n",
    "WatsonX.ai provides a range of foundation models that can be used for various natural language processing tasks. These models are trained on large datasets and can be fine-tuned for specific tasks.\n",
    "\n",
    "**List of Available Models**\n",
    "\n",
    "The available models on WatsonX.ai are listed under the `ModelTypes` class. Here's how you can print the list of available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'LLAMA_2_13B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT', 'FLAN_T5_XL', 'GRANITE_13B_CHAT_V2', 'GRANITE_13B_INSTRUCT_V2', 'ELYZA_JAPANESE_LLAMA_2_7B_INSTRUCT', 'MIXTRAL_8X7B_INSTRUCT_V01_Q', 'CODELLAMA_34B_INSTRUCT_HF', 'GRANITE_20B_MULTILINGUAL']\n"
     ]
    }
   ],
   "source": [
    "from ibm_watsonx_ai.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will output a list of available models, including `FLAN_T5_XXL`, `FLAN_UL2`, `MT0_XXL`, and more.\n",
    "\n",
    "**Choosing a Model**\n",
    "\n",
    "For this example, let's choose the `ibm/granite-13b-instruct-v2` model.\n",
    "\n",
    "**Defining the Model Parameters**\n",
    "\n",
    "We need to define the model parameters for the `ibm/granite-13b-instruct-v2` model. We can do this using the `GenTextParamsMetaNames` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 1000,\n",
    "    GenParams.MIN_NEW_TOKENS: 50,\n",
    "    GenParams.TEMPERATURE: 0.7,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Initializing the WatsonXLLM Class**\n",
    "\n",
    "Next, we need to initialize the `WatsonxLLM` class, which is a wrapper around the WatsonX.ai models that provides LangChain integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ibm import WatsonxLLM\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "# Tool definition\n",
    "@tool\n",
    "def websearch(query, num_results=10):\n",
    "    \"\"\"\n",
    "    Perform a web search for the given query, retrieve descriptions and first paragraphs\n",
    "    from the results, and print them in a readable format. Also, append all descriptions\n",
    "    into a single string and print it.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query.\n",
    "        num_results (int): The number of search results to retrieve. Default is 10.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    all_descriptions = \"\"\n",
    "    results_data = []\n",
    "    results = search(query, num_results=num_results)\n",
    "\n",
    "    for url in results:\n",
    "        try:\n",
    "            response = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            meta_description = soup.find('meta', attrs={'name': 'description'})\n",
    "            description_content = meta_description['content'].strip() if meta_description else 'No description available.'\n",
    "            first_paragraph = soup.find('p')\n",
    "            first_sentence = first_paragraph.text.strip()[:500] if first_paragraph else 'No paragraph available.'\n",
    "            all_descriptions += description_content + \"\\n\"\n",
    "            results_data.append({\n",
    "                'url': url,\n",
    "                'description': description_content,\n",
    "            })\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to retrieve {url}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while processing {url}: {e}\")\n",
    "\n",
    "    return all_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an expert in Artificial Intelligence with a focus on Neural Networks. My primary… · Esperienza: IBM · Formazione: Università degli Studi di Genova · Località: Milano · 266 collegamenti su LinkedIn. Vedi il profilo di Ruslan Magana Vsevolodovna, PhD su LinkedIn, una community professionale di 1 miliardo di utenti.\\nWe talk about Machine Learning, Generative AI, Data Science and Cloud Development\\nAbout Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist.\\nI am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\\nI’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\\nRead writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\\nNo description available.\\nNo description available.\\nNo description available.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "websearch.invoke(\"Who is Ruslan Magana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ev?Ruslan Maganaev (born 5 November 1982) is a Russian professional ice hockey winger currently playing for Yaroslavl Vityaz. He previously played for Vityaz Podolsk. In the Russian Superleague he played for Traktor Chelyabinsk, Salavat Yulaev Ufa and Metallurg Magnitogorsk.\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Who is Ruslan Magana\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods\n",
    "\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 200,\n",
    "    GenParams.MIN_NEW_TOKENS:  10,\n",
    "    GenParams.TEMPERATURE: 0.5,\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "from langchain_ibm import WatsonxLLM\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'websearch(query, num_results=10) - Perform a web search for the given query, retrieve descriptions and first paragraphs\\nfrom the results, and print them in a readable format. Also, append all descriptions\\ninto a single string and print it.\\n\\nArgs:\\n    query (str): The search query.\\n    num_results (int): The number of search results to retrieve. Default is 10.\\n\\nReturns:\\n    None'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.render import render_text_description\n",
    "rendered_tools = render_text_description([websearch])\n",
    "rendered_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: Human, arguments: Who is Ruslan Magana\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "system_prompt = f\"\"\"You are an assistant that has access to the following set of tools. \n",
    "Here are the names and descriptions for each tool:\n",
    "{rendered_tools}.\n",
    "Given the user input, return the name and input of the tool to use. \n",
    "Always return your response as a JSON blob with 'name' and 'arguments' keys.\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_prompt), (\"user\", \"{input}\")]\n",
    ")\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from operator import itemgetter\n",
    "llm_chain = prompt | llm\n",
    "result = llm_chain.invoke({\"Who is Ruslan Magana\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As the assistant, you have access to a variety of tools. Here's a list of available tools: websearch(query, num_results=10) - Perform a web search for the given query, retrieve descriptions and first paragraphs\n",
      "from the results, and print them in a readable format. Also, append all descriptions\n",
      "into a single string and print it.\n",
      "\n",
      "Args:\n",
      "    query (str): The search query.\n",
      "    num_results (int): The number of search results to retrieve. Default is 10.\n",
      "\n",
      "Returns:\n",
      "    None. \n",
      "When a user poses a question, your objective is to select the most suitable tool and provide the necessary input. \n",
      "Your response should be in the form of a JSON object containing the tool's name and its corresponding arguments.\n",
      "For example, if the user asks: \"Who is Ruslan Magana\", you should respond with a JSON object like this:\n",
      "{\n",
      "    \"name\": \"websearch\",\n",
      "    \"arguments\": {\n",
      "        \"query\": \"Who is Ruslan Magana\",\n",
      "        \"num_results\": 10\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"\n",
    "As the assistant, you have access to a variety of tools. Here's a list of available tools: {rendered_tools}. \n",
    "When a user poses a question, your objective is to select the most suitable tool and provide the necessary input. \n",
    "Your response should be in the form of a JSON object containing the tool's name and its corresponding arguments.\n",
    "For example, if the user asks: \"Who is Ruslan Magana\", you should respond with a JSON object like this:\n",
    "{{\n",
    "    \"name\": \"websearch\",\n",
    "    \"arguments\": {{\n",
    "        \"query\": \"Who is Ruslan Magana\",\n",
    "        \"num_results\": 10\n",
    "    }}\n",
    "}}\n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "As the assistant, you have access to a variety of tools. Here's a list of available tools: websearch(query, num_results=10) - Perform a web search for the given query, retrieve descriptions and first paragraphs\n",
      "from the results, and print them in a readable format. Also, append all descriptions\n",
      "into a single string and print it.\n",
      "\n",
      "Args:\n",
      "    query (str): The search query.\n",
      "    num_results (int): The number of search results to retrieve. Default is 10.\n",
      "\n",
      "Returns:\n",
      "    None. \n",
      "When a user poses a question, your objective is to select the most suitable tool and provide the necessary input. \n",
      "Your response should be in the form of a JSON blob containing the tool's name and its corresponding arguments.\n",
      "For example, if the user asks: \"Who is Ruslan Magana\", you should respond with JSON blob format.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = f\"\"\"\n",
    "As the assistant, you have access to a variety of tools. Here's a list of available tools: {rendered_tools}. \n",
    "When a user poses a question, your objective is to select the most suitable tool and provide the necessary input. \n",
    "Your response should be in the form of a JSON blob containing the tool's name and its corresponding arguments.\n",
    "For example, if the user asks: \"Who is Ruslan Magana\", you should respond with JSON blob format.\n",
    "\"\"\"\n",
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "websearch(query, num_results=10)\n"
     ]
    }
   ],
   "source": [
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "# Chain the prompt template with the LLM\n",
    "llm_chain = prompt | llm\n",
    "# Invoke the chain with the input\n",
    "result = llm_chain.invoke({ \"Who is Ruslan Magana\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'websearch', 'arguments': {'query': 'Who is Ruslan Magana', 'num_results': '10'}}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "@tool\n",
    "def json_format(input_string):\n",
    "    \"\"\"input_string : string of the form 'websearch(\"Who is Ruslan Magana\", num_results=10)'\n",
    "    \"\"\"\n",
    "    # Split the input string into function name and arguments\n",
    "    function_name, arguments_str = input_string.split('(', 1)\n",
    "    arguments_str = arguments_str.rstrip(')')\n",
    "\n",
    "    # Split the arguments string into individual arguments\n",
    "    arguments = {}\n",
    "    for arg in arguments_str.split(','):\n",
    "        arg = arg.strip()\n",
    "        if '=' in arg:\n",
    "            key, value = arg.split('=')\n",
    "            key = key.strip('\"')  # Remove quotes around the key\n",
    "            value = value.strip('\"')  # Remove quotes around the value\n",
    "            arguments[key] = value\n",
    "        else:\n",
    "            # If argument doesn't have key=value format, consider it as positional\n",
    "            arguments['query'] = arg.strip('\"')\n",
    "\n",
    "    # Create the JSON object\n",
    "    json_data = {\n",
    "        \"name\": function_name,\n",
    "        \"arguments\": arguments\n",
    "    }\n",
    "\n",
    "    return json_data\n",
    "\n",
    "# Example usage:\n",
    "input_string = 'websearch(\"Who is Ruslan Magana\", num_results=10)'\n",
    "json_data = json_format.invoke(input_string)\n",
    "\n",
    "# Convert the JSON object to a string\n",
    "#json_string = json.dumps(json_data, indent=4)\n",
    "\n",
    "# Print or save the JSON string\n",
    "print(json_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('websearch', {'query': 'Who is Ruslan Magana', 'num_results': '10'})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data['name'] ,json_data['arguments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://it.linkedin.com/in/ruslanmv: 429 Client Error: Request denied for url: https://it.linkedin.com/in/ruslanmv\n",
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n",
      "We talk about Machine Learning, Generative AI, Data Science and Cloud Development\n",
      "About Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist.\n",
      "I am Data Scientist and Data Engineer. I have a Ph.D. in Physics  and I am AWS certified in Machine Learning and Data Analytics - ruslanmv\n",
      "I’m happy to share that I’ve obtained a new certification: watsonx.data Technical Sales Intermediate from IBM!\n",
      "Read writing from Ruslan Magana Vsevolodovna on Medium. Machine Learning Engineer & Data Scientist & Physicist. Every day, Ruslan Magana Vsevolodovna and thousands of other voices read, write, and share important stories on Medium.\n",
      "Play Ruslan Magana Vsevolodovna and discover followers on SoundCloud | Stream tracks, albums, playlists on desktop and mobile.\n",
      "No description available.\n",
      "No description available.\n",
      "‪National Institute for Nuclear Physics‬ - ‪‪445 citazioni‬‬ - ‪Nuclear Physics‬ - ‪Machine Learning‬ - ‪Data Science‬ - ‪Cloud Computing‬ - ‪Big Data‬\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from operator import itemgetter\n",
    "# Create the prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "# Chain the prompt template with the LLM\n",
    "llm_chain = prompt | llm | json_format | itemgetter(\"arguments\") | itemgetter(\"query\") |websearch\n",
    "# Invoke the chain with the input\n",
    "result = llm_chain.invoke({ \"Who is Ruslan Magana\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "template2 = \"You are a chatbot you should answer this question {question} and summarize the following websearch results: {result_websearch}. Answer:\"\n",
    "prompt2 = PromptTemplate.from_template(template2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WatsonxLLM parameters and model initialization\n",
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: DecodingMethods.SAMPLE.value,\n",
    "    GenParams.MAX_NEW_TOKENS: 500,\n",
    "    GenParams.MIN_NEW_TOKENS: 50, # Ensure a minimum length for responses\n",
    "    GenParams.TEMPERATURE: 0.7, # Lower temperature for more focused output\n",
    "    GenParams.TOP_K: 50,\n",
    "    GenParams.TOP_P: 1\n",
    "}\n",
    "model_id = \"ibm/granite-13b-instruct-v2\"\n",
    "watsonx_llm = WatsonxLLM(\n",
    "    model_id=model_id,\n",
    "    url=url,\n",
    "    apikey=api_key,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna: 403 Client Error: Forbidden for url: https://www.researchgate.net/profile/Ruslan-Magana-Vsevolodovna\n",
      "No description available.\n",
      "Ruslan Magana is a Data Scientist and Data Engineer. He has a Ph.D. in Physics and is AWS certified in Machine Learning and Data Analytics. Ruslan Magana has a profile on Medium where he writes about Machine Learning, Generative AI, Data Science and Cloud Development. Ruslan Magana also has a profile on SoundCloud where he uploads his own music. Ruslan Magana is also a Nuclear Physics expert.\n"
     ]
    }
   ],
   "source": [
    "# Chain the prompt template with the LLM\n",
    "llm_chain = prompt | llm | json_format | itemgetter(\"arguments\") | itemgetter(\"query\") |websearch | (lambda result: prompt2.format(question=question, result_websearch=result)) | watsonx_llm\n",
    "# Invoke the chain with the input\n",
    "# Example usage with LangChain chain\n",
    "question = \"Who is Ruslan Magana\"\n",
    "result = llm_chain.invoke({question})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Creating the Chatbot Node**\n",
    "\n",
    "Now that we have the `WatsonxLLM` object, we can create a chatbot node that uses this model to respond to user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_chain.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This code defines a `chatbot` function that takes the current `State` as input and returns an updated `messages` list. \n",
    "\n",
    "The `chatbot` function uses the `llm` object to generate a response to the user's input, and returns a new `messages` list with the response appended to it.\n",
    "\n",
    " Finally, we add the `chatbot` node to our graph using the `add_node` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.set_entry_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, set a `finish` point. This instructs the graph **\"any time this node is run, you can exit.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder.set_finish_point(\"chatbot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll want to be able to run our graph. To do so, call \"`compile()`\" on the graph builder. This creates a \"`CompiledGraph`\" we can use invoke on our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the graph using the `get_graph` method and one of the \"draw\" methods, like `draw_ascii` or `draw_png`. The `draw` methods each require additional dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gHYSUNDX1BST0ZJTEUAAQEAAAHIAAAAAAQwAABtbnRyUkdCIFhZWiAH4AABAAEAAAAAAABhY3NwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAQAA9tYAAQAAAADTLQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAlkZXNjAAAA8AAAACRyWFlaAAABFAAAABRnWFlaAAABKAAAABRiWFlaAAABPAAAABR3dHB0AAABUAAAABRyVFJDAAABZAAAAChnVFJDAAABZAAAAChiVFJDAAABZAAAAChjcHJ0AAABjAAAADxtbHVjAAAAAAAAAAEAAAAMZW5VUwAAAAgAAAAcAHMAUgBHAEJYWVogAAAAAAAAb6IAADj1AAADkFhZWiAAAAAAAABimQAAt4UAABjaWFlaIAAAAAAAACSgAAAPhAAAts9YWVogAAAAAAAA9tYAAQAAAADTLXBhcmEAAAAAAAQAAAACZmYAAPKnAAANWQAAE9AAAApbAAAAAAAAAABtbHVjAAAAAAAAAAEAAAAMZW5VUwAAACAAAAAcAEcAbwBvAGcAbABlACAASQBuAGMALgAgADIAMAAxADb/2wBDAAMCAgMCAgMDAwMEAwMEBQgFBQQEBQoHBwYIDAoMDAsKCwsNDhIQDQ4RDgsLEBYQERMUFRUVDA8XGBYUGBIUFRT/2wBDAQMEBAUEBQkFBQkUDQsNFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBQUFBT/wAARCADaAGIDASIAAhEBAxEB/8QAHQABAAMBAQADAQAAAAAAAAAAAAUGBwgEAQMJAv/EAE4QAAEDAwEDBgkGBxADAQAAAAECAwQABQYRBxIhExYxVZTRCBciQVF0k7ThFBU4YXWyNTZSVmJxgQkYIyQyMzdCRlSCkZKxs9JyhJWh/8QAGgEBAAIDAQAAAAAAAAAAAAAAAAQFAQIDBv/EADcRAAIAAwQGBgkFAQAAAAAAAAABAgMRBBMhkRIVMUFRUgUUYXGhsSIyNGJygcHR8DNCU2Phwv/aAAwDAQACEQMRAD8A/VFa0tpKlEJSkakk6ACo3nVZeuIHaUd9Mq/Fi8epvfcNZZYLBbF2K3KVboilGM2SSwnU+SPqrjPny7NLUcabq6YE2z2e/rjShqfOqy9cQO0o76c6rL1xA7SjvrO+b1r6th+wR3U5vWvq2H7BHdVfrWz8kWaJmrve8DROdVl64gdpR3051WXriB2lHfWd83rX1bD9gjupzetfVsP2CO6mtbPyRZoau97wNE51WXriB2lHfTnVZeuIHaUd9Z3zetfVsP2CO6nN619Ww/YI7qa1s/JFmhq73vA0TnVZeuIHaUd9OdVl64gdpR31nfN619Ww/YI7qc3rX1bD9gjuprWz8kWaGrve8DROdVl64gdpR317Yc+NcWi7EkNSmgd0rZWFjX0aisu5vWvq2H7BHdU3skjtRWsoaZbQy0m7nRDaQlI/isfoAqbZrXKtekoE00q404pfUjWiyXEGlWpfaUpUkryLyr8WLx6m99w1nePfgC2+rNfcFaJlX4sXj1N77hrO8e/AFt9Wa+4KqelfZ4PifkXPR37iQpSleVLopETbRh9wyOfYol1XKucEvIfbYhPuIC2klTraXAgoWtIB1QlRVrw01qvbNPCJsGdbPZeUzm5VlZhb6paHoUnk208sttvccU0kPEhA1DepBOhANVfFfnWw7b/kWJWfJ7fjdwuE5/IYd5gFFtQvdUUy4j587roSdxKiCFklKCKgccuGZ4rsJnYharDkVtyazTXEypTFuKuUiLuClOuQnFAoec5BwqSBqdQeGoFTrqClF2b++u4h3kVavt3dxsdt244TdsWvmRRr1ra7IkruSnIj7b0Ubu9qtlSA4NRxHk8fNrVazPwmcax22WafbkTbxFn3iPbFSGrdL5MIcOqnWlBkh7RPFIRrvE8CdNKxy54pdJNo22JtWP5nIiX3Foqbc7fmZL8qa60X0rSOU3lpVq4ndbUEq01ITu1s222yz04Nh8u2WmVcU4/fbZcpEC3slx/5OysBYbbHFSkg67o48KzdSoYktte3sX1F5Mihb4fc1K1XNi9WyLPi8r8mlNJeb5ZlbK91Q1G8hYCknQ9CgCPOK9dR9hvKMgtEa4txZkJEhO8GJ8dTD6OJHltqAKTw6DUhUF4MlrFCvdsr/tV9sH3WPXhr3bK/7VfbB91j1f8AQ/rzfh/6hK63/pLvL1SlK9AeeIvKvxYvHqb33DWc2NpD+OW9txIW2uI2lSVDUEFA1BrU5sRufDfiuglp9tTawDodCND/AL1TWdklujsoabu16Q2hISlIm8ABwA6KjWqzK1SlBpUadSwstohkV0t5mI8H/ZmCCMAxsEecWtn/AK0/e/bMvzAxv/5bP/WtR8VUHri99t+FPFVB64vfbfhVdqyZ/N5kzrkjl8ERLDDcZhtllCWmm0hCEIGgSkDQAD0V9lSXiqg9cXvtvwp4qoPXF77b8K56n/tWTOmsJXBkbSs08FOLN2u7FLVk2Q3u6OXSRKmNOKjyOTRutyXG0aJA/JSK13xVQeuL3234U1P/AGrJjWErgzPb7sdwXKLq/c7xh9kulxf3eVly4DTjrmiQkbyikk6AAfqArwq2BbNFhIVgWOKCRokG2M8BrroPJ9JP+dah4qoPXF77b8KeKqD1xe+2/Cui6LjWCneZp12Q/wBvgis45i9nxC2Jt1jtcS0QEqKxFhMpabCj0ndSANTVi2V/2q+2D7rHr7PFVB64vfbfhU7i+KxMSiyWIjsh75S+ZDrkpzlFqWUpT0/qQkfsqbY7H1RxxOPScSpv4p/QjWm1QTpehCiZpSlTSrFKUoBSlKAUpSgOd/AE+jJYfXrl769XRFc7+AJ9GSw+vXL316uiKAUpSgFKUoBSlKAUpSgFKUoBSlKA538AT6Mlh9euXvr1dEVzv4An0ZLD69cvfXq6IoBSlKAUpSgFKUoBSlfBISCSQAOJJoD5pVGnbTBIcKLBbzdmwQPlzr3IxVfWhWilOD60p3Tw0V6I85plquIiWVH6JceVp+3Qf7V3uWvWaXeyTDZpsSqoTSa4i/dO9hasuwG37RbZHC7njo+TT9weU5CWvyT6TybitdPQ6snorpPnnl392sn+p6vDe7zkOSWafablbrDMt05hcaTHcLxS62tJSpJ+ogkUulzLM26pO4H5ufueGxFzaltziX+W0v5jxJTdzdcHAKlBWsZvX076Sv8AU0R56/XuuZ/B82Zz/BzwZzG7Ai2TEvy3JkibLLnKvLVoBrugABKEpSAOHAnpJrTueeXf3ayf6nqXS5lmOqTuBpVKzUZnluo1jWXTz6Ker0Rtot5hqBudhafj/wBZ21SeUcT9fJuJTqP/ABUT6AfOuq7Ik/n9zDss5Y6JoVK8NmvcLIICJlvkJkR1EjUApUlQ6UqSQClQ86SAR5xXuri04XRkXYKUpWAKzzOLsq93hdgbURb4yEuT90/z6lA7jCv0d3y1D+tqgcUlQOh1kUBanrxkjrn86q6vBXp0SEoTr/gSn9mldoPRhijW1bPmTbJAo5mO49/RXlut2g2K3SLhcpke3wI6d96VKdS000n0qUogAfWaqe2rO5WzTZjfMigx2pM+MhtqM2+SG+WddQy2V6cd0KcBOnmB6KzHbdjGV2HwftobuRZo5lCXbMocgu2sRUsuajUoLYB3fNuq3j+lUQvI49GtFsR0ICCNRxFKxOLlWTbPtoC7NlOXsXWzzcbl3n5c7b244trkdbYWUhH8prdd10WVKG5/KOtVzZptFzO6Z9Bx+4Xq9zLRkdlly7fdrrZYkB5p1st7r0dCCrVBS6Duvo11CekEihreqtKHQdxvVus8Bc6fPiwoSFJQqTIeS22lSlhCQVEgalRCQPOSB017K44tVnucTwG2HncgkzkzFW0xWJEdkNwSLm2DubiEqWCSCd9Sjw4EVf8AMNqOYbC7vf4d7u6M2jKxuXfLc89DbjPMPsLQgtOBoBKmjyqTvaBQ0I49NZoaqdhVrCiOiK/lDiXU7yFBadSNUnUag6GsPjXTO8SzDDrHfMy+e0ZjFmRy81bo7K7XLbjF5LjG6khaNAsbroXxCSSQSK+7wP7TPgbDsdky75KuceVGCo8R9llCIYDjgIQpCEqVvagnfKjw4aVg3UysWjT8w+5sKrmrEp3z20SmMnQXBre0QtngC4R+U2PK16SkFPo01gEEajiKy2Yy3IiPtO6FpaFJXr0aEaGrls7kvTNn+MyJBJfdtkVbhPSVFpJP/wC1K9eVpPanT5PZlRlXboEolEt5YaUpXIqxWZZFAVYcwkrUCIV5KXmVk+SJCWwlbf1EobCx6dHD5iTpteO72iJfbe7CmtB6O5pqNSCkg6hSVDilQIBChoQQCDqK6QRJVUWxneTNcqNRGUZVi9szXHLhYrzFTNtc9ksSGFEjeSfQRxBHAgjiCARVCd2Aw52K3vHrpmGWXq33SCbeoXGe24phrUHVv+CAK+AG+sKOnnrVJ2M5DY3N1pgZDCBAS60tDUpI/TSrdQo/Wkp1/JHnjjOuKeCsbvSVecCMlWn7QoilxG/Vo+5/jLxTZMzGpXcp2VWPMr4i5XT5Q/paJdkXFCwGXY8nc5Xe4b29/BgAhQ01PDo0hcZ2EW7G8lsd9XkeR3e42Zh2JFVc5jbiBHWgJLJSltI0GiVbwAWShO8ogaVfPnCf+bl67J8afOE/83L12T406vN4G2nJbrVGcN+DlYWsQuuKi95AcbmvNvNW0zEcnB3JIkBLCuT30grTpxUogdBB0IkbLsMsUJ+8ybxOuuXTLrBVa35N/kpeWmGrUqYQEJQlKCTqdBqSASeFTmIbQoefWJm9Y9brpdrW8tbbcqPF1QpSFFCwNT5lJI/ZU184T/zcvXZPjTq83gY0pPFFJw7YfacRv8C8O3m+5DLtkZcS2C9y0vJt7SgAsNBKE8SEpSVL3laDTWpPZzsug7MGZcS1XW6v2p1ZVHtk19DkeCCtSylnRAUEkrPBSldA0qxifPJA5uXof+p8a9EaHkd1UERLA7BSemTdXkNoT/gQpa1H6tEg+kcdHV5m9U72heSYcao815ZfuTKbTDUROuOsdtSTxaQeC3f1ISSf17o84rWIkVqDFZjMJCGWUJbQkeZIGgH+QqGxfEWsdS4+898vujw0emqQEEj8hCeO4gHoTqfSSTxqfrMTShUEO7x/wprTPvosNiFKUrkRBSlKAUpSgFKUoDnfwBPoyWH165e+vV0RXO/gCfRksPr1y99eroigFKUoBSlKAUpSgFKUoBSlKAUpSgOd/AE+jJYfXrl769XRFc7+AJ9GSw+vXL316uiKAUpSgFKUoBSlKAUpXjm3m321xLcudGirUN4JeeSgkenQmspOJ0QPZSovnVZeuIHaUd9OdVl64gdpR31vdx8rM0ZKVzf4WXhc3LwXbhYd7A+ctnuzS9y4Ju3yXk30HymlI5Bf9VSFA7w11UNPJJrfOdVl64gdpR31kvhS7Pse297F75jIudsN1Sj5ZanVyWxyctsEo468AoFTZPmDhpdx8rFGcl+A/wCGRPg8ztj9uwBV1el3J7lLqi7bnIsuvreddLXInUNoUo6b43tzza1+kVcAfuZ+x+Dhlsve0LJXGLfeZqlWy3RpriW3GmEqHLObqjqCtaQkagEBtXmVXdfOqy9cQO0o76XcfKxRkpSovnVZeuIHaUd9OdVl64gdpR30u4+VijJSlRreS2h5xDbd1hLcWQlKUyEEknoAGtSVauFw7UYFKUrUCsszGBFn7THxJjMyAm0R93lWwrT+Gf6Na1Os0yb+kyT9kRv+aRSNuGTMa4fVFd0i2rJG12eaPFzetfVsP2CO6nN619Ww/YI7qkKV5y9mczzPB6cXEj+b1r6th+wR3U5vWvq2H7BHdXnyvL7Pg9mcut8nt2+ChSUcosFRUtR0ShCUgqWonoSkEnzCqyzt3wR3HJl9OQNsW2FJZiS1yWHWXIzrqkpbDra0BbYUVDylJA01OugJrZTJrxTfidFexKqr4lu5vWvq2H7BHdTm9a+rYfsEd1QON7WMUyuPd3oF1CE2hAcnpnMOw1xmykqDi0vJQoIKUqIXpukA6HhVNsPhC2nONq2O45i8pq42qbbZsyU+7DkMuAtqZDRaLgSFNq33PKAUDujQjQ65UU7HF4d5soJzrtw27e81Dm9a+rYfsEd1Ob1r6th+wR3VIUrS9mczzOOnFxK5kNmt8WLDdZgxmnU3GDotDKUkfxproIFbXWQZR+D4v2jB97arX6vrNFFFZU4nX0ovKE9j0O27M68z8kKUpXUvBWaZN/SZJ+yI3/NIrS6zTJv6TJP2RG/5pFazP0Jvd9UVvSPskz5eaP7pVaynZpiWcSmZOQ41ar3IZRybbtwhtvKQnXXdBUDoNTrUL+9/2Z6Acwcc0HHT5sZ0+7Xm1o72eFSgpi3l/pWPCSxO5XyPht3hwrtdYFivHyu4QLFJcYmrZUy40XGVNqSsrQVg7qSCQVCqRf8ACLfdsFuV1xrGszTc5l9srUhWSGY/LksR5jTm+lD61uJbQFuakhOmij0ca37FMAxnBRKGOWC22ISt0vi3xUM8ru67u9uga6bytNfSan66KZopJbiRDaHAlDDsXy31xXec3bc9nmRZll20VizW2Q8J+G29plZQUMy3mpz7q44cI3d9Tfk6a8A4NdAambLkMrPtuGDXaLiOSWK226yXJiQu72pyK2y4tUbda1I018hWhHA6eSTodN4rwXyxW7JrVItl2gx7nbpAAeiy2g424AQRvJPA8QD+yl5hRr82BWj0VC1+NUZ76VQUbAdmjZ1TgOOJOhGotjI4EaEfyfRX3W/Ybs7tM+NOhYPj8SZGdS8w+zbmkrbWkgpUkhOoIIBBHorn6PE4Ul8Xl/pYco/B8X7Rg+9tVr9ZBlH4Pi/aMH3tqtfq/snsq+KLyhPXdDezP4n5IUpSu5eiqzkOAQMiuwuTsqdElBhMcqhv8mFISpSgCND51q/zqzUraGJw7DDSiVGqopPiqg9cXvtvwp4qoPXF77b8Ku1K2vH2ZI5XMrkWSKT4qoPXF77b8KeKqD1xe+2/CrtSl4+zJC5lciyRSfFVB64vfbfhTxVQeuL3234VdqUvH2ZIXMrkWSKT4qoPXF77b8KeKqD1xe+2/CrtSl4+zJC5lciyRSRsnthdYW7cbtISy82+G3peqCpCwtOo04jVIq7UpWIo3EqM6QwwwKkKoKUpWhsf/9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testig our Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "    for event in graph.stream({\"messages\": (\"user\", user_input)}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
